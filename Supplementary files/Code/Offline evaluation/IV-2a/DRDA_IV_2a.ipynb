{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction"
      ],
      "metadata": {
        "id": "PKC7c8J-dRv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run each cell one by one**"
      ],
      "metadata": {
        "id": "vnJgq3X2dR0R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU1wHT_tnG4s"
      },
      "source": [
        "#Load the dataset from the google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R1nShhmh_vPm"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pprint, pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "id_1_train=\"1GFFtOkPJskSjhA7-Klfj33fF5eVWa5sE\"\n",
        "id_1_evaluation=\"1G7cd67MlLEBn-OoY612T08yBoRoGyXyd\"\n",
        "\n",
        "\n",
        "id_2_train=\"1AP5V-adbxm-bdOuB-7EtgegqzqtlldxZ\"\n",
        "id_2_evaluation=\"1-XoFS7lPF9DQSO4ItK_KtorUaLFmA4Pn\"\n",
        "\n",
        "id_3_train=\"1RFuUrqJPqBWycC_j1b2Vy1DgBr1yQbPG\"\n",
        "id_3_evaluation=\"1fPSYHnTi5ORcI7QgXjIduAYOYZsoE9tI\"\n",
        "\n",
        "id_4_train=\"1jovz85ALrSRBZgLNUeeoihSJkEYJT3ZH\"\n",
        "id_4_evaluation=\"1e6C3vn7sPV-LO-Q1EbLTHvlcT7oqhJ99\"\n",
        "\n",
        "\n",
        "id_5_train=\"17wrRD332sGeRob6W36MNkQmj9Kc6u5lG\"\n",
        "id_5_evaluation=\"1JUc3flvygXmhdTL-kM_9pvh25pKfpvNy\"\n",
        "\n",
        "id_6_train=\"1vROntA0I75Gq85VXa9jwukl-IXxEOCCb\"\n",
        "id_6_evaluation=\"1pYnce5N2KxxNmFM2sgfxldLpaKgclEiY\"\n",
        "\n",
        "id_7_train=\"1_qvjVphyrE311iInuVDhhWWWN3BC4-sm\"\n",
        "id_7_evaluation=\"18V-Tp3Kny9aYz8rxSFsRQrYDtstVF75d\"\n",
        "\n",
        "\n",
        "id_8_train=\"1Cpi6yNpnMX5oCClCClv7Lcgg2RvxspUF\"\n",
        "id_8_evaluation=\"13cz0_VMYI0-u2LlEhmy61mGuzmjbmP1B\"\n",
        "\n",
        "id_9_train=\"1o0gyOZrYsyw68GFhpKFuwoFRn4pVwb7Q\"\n",
        "id_9_evaluation=\"1KW0_TzquGXygyBksatKEvevm6NYwddEA\"\n",
        "\n",
        "\n",
        "\n",
        "def load(id_train,id_evaluation,subject):\n",
        "  downloaded = drive.CreateFile({'id':id_train}) \n",
        "  downloaded.GetContentFile('A0'+str(subject)+'T.mat')\n",
        "  downloaded = drive.CreateFile({'id':id_evaluation}) \n",
        "  downloaded.GetContentFile('A0'+str(subject)+'E.mat')\n",
        "\n",
        "load(id_1_train,id_1_evaluation,1)\n",
        "load(id_2_train,id_2_evaluation,2)\n",
        "load(id_3_train,id_3_evaluation,3)\n",
        "\n",
        "load(id_4_train,id_4_evaluation,4)\n",
        "load(id_5_train,id_5_evaluation,5)\n",
        "load(id_6_train,id_6_evaluation,6)\n",
        "\n",
        "load(id_7_train,id_7_evaluation,7)\n",
        "load(id_8_train,id_8_evaluation,8)\n",
        "load(id_9_train,id_9_evaluation,9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GvrugCjnZfb"
      },
      "source": [
        "#Load the package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-pAVSZXaoOqp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd.function import Function\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.utils.data as Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOeNys81qFsk"
      },
      "source": [
        "#Convert the data in mat format into Numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ldAa6XJhA17Q"
      },
      "outputs": [],
      "source": [
        "def load_all_data (crossValidation, data_path): \n",
        "\n",
        "    big_X_train, big_y_train, big_X_test, big_y_test = [None]*9, [None]*9, [None]*9, [None]*9\n",
        "    for subject in range (0,9):\n",
        "        path = data_path+'s' + str(subject+1) + '/'\n",
        "        big_X_train[subject], big_y_train[subject] = get_data(subject+1, True ,path)\n",
        "        big_X_test[subject], big_y_test[subject] = get_data(subject+1, False ,path)\n",
        "    \n",
        "    return big_X_train, big_y_train, big_X_test, big_y_test\n",
        "\n",
        "def get_data_2(subject,training,path, highpass = False):\n",
        "\t'''\tLoads the dataset 2a of the BCI Competition IV\n",
        "\tavailable on http://bnci-horizon-2020.eu/database/data-sets\n",
        "\n",
        "\tKeyword arguments:\n",
        "\tsubject -- number of subject in [1, .. ,9]\n",
        "\ttraining -- if True, load training data\n",
        "\t\t\t\tif False, load testing data\n",
        "\t\n",
        "\tReturn:\tdata_return \tnumpy matrix \tsize = NO_valid_trial x 22 x 1750\n",
        "\t\t\tclass_return \tnumpy matrix \tsize = NO_valid_trial\n",
        "\t'''\n",
        "\tNO_channels = 22\n",
        "\tNO_tests = 6*48 \t\n",
        "\tWindow_Length = 7*250 \n",
        "\n",
        "\tclass_return = np.zeros(NO_tests)\n",
        "\tdata_return = np.zeros((NO_tests,NO_channels,Window_Length))\n",
        "\n",
        "\tNO_valid_trial = 0\n",
        "\tif training:\n",
        "\t\ta = sio.loadmat(path+'A0'+str(subject)+'T.mat')\n",
        "\telse:\n",
        "\t\ta = sio.loadmat(path+'A0'+str(subject)+'E.mat')\n",
        "\ta_data = a['data']\n",
        "\tfor ii in range(0,a_data.size):\n",
        "\t\ta_data1 = a_data[0,ii]\n",
        "\t\ta_data2= [a_data1[0,0]]\n",
        "\t\ta_data3= a_data2[0]\n",
        "\t\ta_X \t\t= a_data3[0]\n",
        "\t\ta_trial \t= a_data3[1]\n",
        "\t\ta_y \t\t= a_data3[2]\n",
        "\t\ta_fs \t\t= a_data3[3]\n",
        "\t\ta_classes \t= a_data3[4]\n",
        "\t\ta_artifacts = a_data3[5]\n",
        "\t\ta_gender \t= a_data3[6]\n",
        "\t\ta_age \t\t= a_data3[7]\n",
        "\n",
        "\t\tfor trial in range(0,a_trial.size):\n",
        "\t\t\tif(a_artifacts[trial]==0):\n",
        "\t\t\t\tdata_return[NO_valid_trial,:,:] = np.transpose(a_X[int(a_trial[trial]):(int(a_trial[trial])+Window_Length),:22])\n",
        "\t\t\t\tclass_return[NO_valid_trial] = int(a_y[trial])\n",
        "\t\t\t\tNO_valid_trial +=1\n",
        "\n",
        "\treturn data_return[0:NO_valid_trial,:,:], class_return[0:NO_valid_trial]\n",
        "\n",
        "def prepare_features(path,subject,crossValidation=False):\n",
        "    fs = 250 \n",
        "    # t1 = int(1.5*fs)\n",
        "    t1 = int(2*fs)\n",
        "\n",
        "    t2 = int(6*fs)\n",
        "    T = t2-t1\n",
        "    X_train, y_train = get_data_2(subject+1,True,path)\n",
        "    # X_train, y_train,train_artifact = get_data_2(subject+1,True,path)\n",
        "\n",
        "    if crossValidation:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=0)\n",
        "    else:\n",
        "        X_test, y_test = get_data_2(subject+1,False,path)\n",
        "\n",
        "    # prepare training data \t\n",
        "    N_tr,N_ch,_ =X_train.shape \n",
        "    X_train = X_train[:,:,t1:t2].reshape(N_tr,1,N_ch,T)\n",
        "    y_train_onehot = (y_train-1).astype(int)\n",
        "    y_train_onehot = to_categorical(y_train_onehot)\n",
        "    # prepare testing data \n",
        "    N_test,N_ch,_ =X_test.shape \n",
        "    X_test = X_test[:,:,t1:t2].reshape(N_test,1,N_ch,T)\n",
        "    y_test_onehot = (y_test-1).astype(int)\n",
        "    y_test_onehot = to_categorical(y_test_onehot)\t\n",
        "\n",
        "    return X_train,y_train,y_train_onehot,X_test,y_test,y_test_onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loader"
      ],
      "metadata": {
        "id": "R_KTn3hHQtI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(Subject_number):\n",
        "  #Prepare the raw feature data for training and testing#\n",
        "  #You can change the \"Subject_number\" (0-8 corresponding to B01-B09, respectively) to try training and testing between different subjects\n",
        "  Subject_number=Subject_number\n",
        "  BATCH_SIZE = 128\n",
        "  x_train,y_train,y_train_onehot,x_test,y_test,y_test_onehot = prepare_features('',Subject_number,False)\n",
        "  \n",
        "  total_x=np.vstack((x_train,x_test))\n",
        "  total_y=np.hstack((y_train,y_test))\n",
        "  # total_y_onehot=np.vstack((y_train_onehot,y_test_onehot))\n",
        "  index1=np.where(total_y == 1)[0]\n",
        "  index2=np.where(total_y == 2)[0]\n",
        "  index3=np.where(total_y == 3)[0]\n",
        "  index4=np.where(total_y == 4)[0]\n",
        "\n",
        "  hyperparameter=10\n",
        "\n",
        "  choice1=np.random.choice(index1,hyperparameter,replace=False)\n",
        "  choice2=np.random.choice(index2,hyperparameter,replace=False)\n",
        "  choice3=np.random.choice(index3,hyperparameter,replace=False)\n",
        "  choice4=np.random.choice(index4,hyperparameter,replace=False)\n",
        "\n",
        "\n",
        "  choice_total=np.hstack((choice1,choice2,choice3,choice4))\n",
        "  xxxtest= np.delete(total_x, choice_total,0)\n",
        "  yyytest=np.delete(total_y, choice_total)\n",
        "\n",
        "\n",
        "  target_x=total_x[choice_total]\n",
        "  target_y=total_y[choice_total]\n",
        "\n",
        "\n",
        "\n",
        "  total_x_train=[]\n",
        "  total_y_train=[]\n",
        "  full_subjectnum=[0,1,2,3,4,5,6,7,8]\n",
        "  full_subjectnum.remove(Subject_number)\n",
        "  for train_subject in full_subjectnum:\n",
        "    x_train,y_train,y_train_onehot,x_test,y_test,y_test_onehot = prepare_features('',train_subject,False)\n",
        "\n",
        "    if len(total_x_train)==0:\n",
        "      total_x_train=x_train\n",
        "      total_y_train=y_train\n",
        "    else:\n",
        "      total_x_train=np.vstack((total_x_train,x_train))\n",
        "      total_y_train=np.hstack((total_y_train,y_train))\n",
        "    total_x_train=np.vstack((total_x_train,x_test))\n",
        "    total_y_train=np.hstack((total_y_train,y_test))\n",
        "  x_train=total_x_train\n",
        "  y_train=total_y_train\n",
        "\n",
        "\n",
        "  x_test=xxxtest\n",
        "  y_test=yyytest\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  target_x=torch.from_numpy(target_x)\n",
        "  target_y=torch.from_numpy(target_y-1)\n",
        "  target_x=target_x.float()\n",
        "  target_y=target_y.long()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  x_train=torch.from_numpy(x_train)\n",
        "  y_train=torch.from_numpy(y_train-1)\n",
        "  x_test=torch.from_numpy(x_test)\n",
        "  y_test=torch.from_numpy(y_test-1)\n",
        "  x_train=x_train.float()\n",
        "  x_test=x_test.float()\n",
        "  y_train=y_train.long()\n",
        "  y_test=y_test.long()\n",
        "\n",
        "\n",
        "  source_dataset=Data.TensorDataset(x_train,y_train)\n",
        "  target_dataset=Data.TensorDataset(target_x,target_y)\n",
        "  val_dataset=Data.TensorDataset(x_test,y_test)\n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "  source_loader= Data.DataLoader(source_dataset, batch_size=400, shuffle=True, num_workers=2)\n",
        "  target_loader = Data.DataLoader(target_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "  test_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "  return source_loader,target_loader,test_loader"
      ],
      "metadata": {
        "id": "zs7SlW-qQvKW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Wr8L0-x5-w"
      },
      "source": [
        "#EEGNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "\n",
        "class Constrainedlinear(nn.Linear):\n",
        "    def _max_norm(self,w,max_val=0.25,eps=1e-8):\n",
        "        norm = w.norm(2, dim=1, keepdim=True)\n",
        "        desired = torch.clamp(norm, 0, max_val)\n",
        "        return w * (desired / (eps + norm))\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self._max_norm(self.weight),self.bias)\n"
      ],
      "metadata": {
        "id": "MP6XoAC2QEDR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1q37twcQEyeL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self,classes_num):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.numC=22\n",
        "\n",
        "        self.drop_out = 0.2\n",
        "        \n",
        "        self.block_1 = nn.Sequential(\n",
        "            # Pads the input tensor boundaries with zero\n",
        "            # left, right, up, bottom\n",
        "            nn.ZeroPad2d((15, 16, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,          # input shape (1, C, T)\n",
        "                out_channels=8,         # num_filters\n",
        "                kernel_size=(1, 32),    # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (8, C, T)\n",
        "            nn.BatchNorm2d(8)           # output shape (8, C, T)\n",
        "        )\n",
        "        \n",
        "        # block 2 and 3 are implementations of Depthwise Convolution and Separable Convolution\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=8,          # input shape (8, C, T)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(self.numC, 1),    # filter size\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//4)\n",
        "            nn.Dropout(self.drop_out)   # output shape (16, 1, T//4)\n",
        "        )\n",
        "        \n",
        "        self.block_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "               in_channels=16,          # input shape (16, 1, T//4)\n",
        "               out_channels=16,         # num_filters\n",
        "               kernel_size=(1, 16),     # filter size\n",
        "               groups=16,\n",
        "               bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,         # input shape (16, 1, T//4)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(1, 1),     # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T//4)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//32)\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.class_classifier = nn.Sequential()\n",
        "        self.class_classifier.add_module('c_fc1',  Constrainedlinear((16 * 15), classes_num))\n",
        "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax())\n",
        "        self.domain_classifier = nn.Sequential()\n",
        "        self.domain_classifier.add_module('d_fc1', nn.Linear((16 * 15), 100))\n",
        "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
        "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
        "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
        "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def forward(self, x, alpha):\n",
        "\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        \n",
        "        feature = x.view(x.size(0), -1)\n",
        "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
        "        class_output = self.class_classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "\n",
        "        return feature, class_output, domain_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Center loss"
      ],
      "metadata": {
        "id": "DnPZ6scvQTL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    def __init__(self, num_classes, feat_dim, size_average=True):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))\n",
        "        self.centerlossfunc = CenterlossFunc.apply\n",
        "        self.feat_dim = feat_dim\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, label, feat):\n",
        "        batch_size = feat.size(0)\n",
        "        feat = feat.view(batch_size, -1)\n",
        "        # To check the dim of centers and features\n",
        "        if feat.size(1) != self.feat_dim:\n",
        "            raise ValueError(\"Center's dim: {0} should be equal to input feature's \\\n",
        "                            dim: {1}\".format(self.feat_dim,feat.size(1)))\n",
        "        batch_size_tensor = feat.new_empty(1).fill_(batch_size if self.size_average else 1)\n",
        "        loss = self.centerlossfunc(feat, label, self.centers, batch_size_tensor)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CenterlossFunc(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, feature, label, centers, batch_size):\n",
        "        ctx.save_for_backward(feature, label, centers, batch_size)\n",
        "        centers_batch = centers.index_select(0, label.long())\n",
        "        return (feature - centers_batch).pow(2).sum() / 2.0 / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        feature, label, centers, batch_size = ctx.saved_tensors\n",
        "        centers_batch = centers.index_select(0, label.long())\n",
        "        diff = centers_batch - feature\n",
        "        # init every iteration\n",
        "        counts = centers.new_ones(centers.size(0))\n",
        "        ones = centers.new_ones(label.size(0))\n",
        "        grad_centers = centers.new_zeros(centers.size())\n",
        "\n",
        "        counts = counts.scatter_add_(0, label.long(), ones)\n",
        "        grad_centers.scatter_add_(0, label.unsqueeze(1).expand(feature.size()).long(), diff)\n",
        "        grad_centers = grad_centers/counts.view(-1, 1)\n",
        "        return - grad_output * diff / batch_size, None, grad_centers / batch_size, None\n",
        "\n",
        "#Testing function\n",
        "def main(test_cuda=False):\n",
        "    print('-'*80)\n",
        "    device = torch.device(\"cuda\" if test_cuda else \"cpu\")\n",
        "    ct = CenterLoss(10,2,size_average=True).to(device)\n",
        "    y = torch.Tensor([0,0,2,1]).to(device)\n",
        "    feat = torch.zeros(4,2).to(device).requires_grad_()\n",
        "    print (list(ct.parameters()))\n",
        "    \n",
        "    print (ct.centers.grad)\n",
        "    out = ct(y,feat)\n",
        "    print(out.item())\n",
        "    out.backward()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(999)\n",
        "    main(test_cuda=False)\n",
        "    if torch.cuda.is_available():\n",
        "        main(test_cuda=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Fm6uevQVmx",
        "outputId": "f35da214-2538-4b4c-fca8-c1ff7253f07d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "[Parameter containing:\n",
            "tensor([[-0.2528,  1.4072],\n",
            "        [ 0.2910,  1.0365],\n",
            "        [ 0.6396, -1.0857],\n",
            "        [-1.6153,  1.5635],\n",
            "        [ 0.1878, -0.9564],\n",
            "        [-0.2440, -0.4153],\n",
            "        [ 0.3259, -1.6059],\n",
            "        [-0.5272,  0.3401],\n",
            "        [-1.6526,  0.2108],\n",
            "        [-1.3302,  0.4676]], requires_grad=True)]\n",
            "None\n",
            "0.8543729186058044\n",
            "--------------------------------------------------------------------------------\n",
            "[Parameter containing:\n",
            "tensor([[ 0.9147, -1.1896],\n",
            "        [-0.7501, -1.5465],\n",
            "        [-0.4303,  0.3703],\n",
            "        [ 0.4588,  0.2950],\n",
            "        [ 0.2126, -0.1098],\n",
            "        [-0.1349,  2.0038],\n",
            "        [ 1.8339,  2.3544],\n",
            "        [ 0.3990, -0.3584],\n",
            "        [-0.2116,  0.5276],\n",
            "        [ 0.5683, -0.6786]], device='cuda:0', requires_grad=True)]\n",
            "None\n",
            "0.9725638628005981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Function"
      ],
      "metadata": {
        "id": "dTBSDEohQcnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "def test(dataset_name,dataloader, epoch):\n",
        "\n",
        "\n",
        "    cuda = True\n",
        "    cudnn.benchmark = True\n",
        "    alpha = 0\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" training \"\"\"\n",
        "    model_root='/'\n",
        "    my_net = torch.load(os.path.join(\n",
        "        model_root, 'mnist_mnistm_model_epoch_' + str(epoch) + '.pth'\n",
        "    ))\n",
        "    my_net = my_net.eval()\n",
        "\n",
        "    if cuda:\n",
        "        my_net = my_net.cuda()\n",
        "\n",
        "    len_dataloader = len(dataloader)\n",
        "    data_target_iter = iter(dataloader)\n",
        "\n",
        "    i = 0\n",
        "    n_total = 0\n",
        "    n_correct = 0\n",
        "\n",
        "    while i < len_dataloader:\n",
        "\n",
        "        # test model using target data\n",
        "        data_target = data_target_iter.next()\n",
        "        t_img, t_label = data_target\n",
        "\n",
        "        batch_size = len(t_label)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 1, 3, 1000)\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "\n",
        "        if cuda:\n",
        "            t_img = t_img.cuda()\n",
        "            t_label = t_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(t_img).copy_(t_img)\n",
        "        class_label.resize_as_(t_label).copy_(t_label)\n",
        "\n",
        "        _,class_output, _ = my_net(x=input_img, alpha=alpha)\n",
        "        pred = class_output.data.max(1, keepdim=True)[1]\n",
        "        n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
        "        n_total += batch_size\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    accu = n_correct.data.numpy() * 1.0 / n_total\n",
        "\n",
        "    print('epoch: %d, accuracy of the %s dataset: %f' % (epoch, dataset_name, accu))\n",
        "    return accu"
      ],
      "metadata": {
        "id": "jMp-5aMaQeOx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D-sxuh8yI6y"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CL-tE2VoFL_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27113a02-30dc-4375-87c0-786fda7e7399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 499, accuracy of the train dataset: 0.740946\n",
            "epoch: 499, accuracy of the target dataset: 1.000000\n",
            "epoch: 499, accuracy of the test dataset: 0.717899\n"
          ]
        }
      ],
      "source": [
        "Subject_number=0\n",
        "source_loader,target_loader,test_loader=train(Subject_number)\n",
        "\n",
        "\n",
        "\n",
        "cuda = True\n",
        "cudnn.benchmark = True\n",
        "lr = 1e-4*5\n",
        "batch_size = 128\n",
        "n_epoch = 500\n",
        "\n",
        "\n",
        "manual_seed = random.randint(1, 10000)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "my_net = CNNModel(4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "centerloss = CenterLoss(4, 240).cuda()\n",
        "optimzer4center = optim.Adam(centerloss.parameters(), lr =0.5)\n",
        "optimizer = optim.Adam(my_net.parameters(), lr=lr)\n",
        "\n",
        "loss_class = torch.nn.NLLLoss()\n",
        "loss_domain = torch.nn.NLLLoss()\n",
        "\n",
        "if cuda:\n",
        "    my_net = my_net.cuda()\n",
        "    loss_class = loss_class.cuda()\n",
        "    loss_domain = loss_domain.cuda()\n",
        "\n",
        "for p in my_net.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "source_loader,target_loader,test_loader=train(Subject_number)\n",
        "dataloader_source=source_loader\n",
        "dataloader_target=target_loader\n",
        "\n",
        "model_root = os.path.join('')\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n",
        "    data_source_iter = iter(dataloader_source)\n",
        "    data_target_iter = iter(dataloader_target)\n",
        "\n",
        "    i = 0\n",
        "    while i < len_dataloader:\n",
        "\n",
        "        p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "        # training model using source data\n",
        "        data_source = data_source_iter.next()\n",
        "        s_img, s_label = data_source\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        my_net.zero_grad()\n",
        "        batch_size = len(s_label)\n",
        "        input_img = torch.FloatTensor(batch_size, 1, 22, 1000)\n",
        "\n",
        "\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "        domain_label = torch.zeros(batch_size)\n",
        "        domain_label = domain_label.long()\n",
        "\n",
        "        if cuda:\n",
        "            s_img = s_img.cuda()\n",
        "            s_label = s_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(s_img).copy_(s_img)\n",
        "        class_label.resize_as_(s_label).copy_(s_label)\n",
        "\n",
        "\n",
        "        _,class_output, domain_output = my_net(x=input_img, alpha=alpha)\n",
        "\n",
        "        err_s_label = loss_class(class_output, class_label)\n",
        "        err_s_domain = loss_domain(domain_output, domain_label)\n",
        "\n",
        "\n",
        "        data_target = data_target_iter.next()\n",
        "        t_img, t_label = data_target\n",
        "\n",
        "        batch_size = len(t_img)\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "\n",
        "\n",
        "        centerloss.zero_grad()\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 1, 22, 1000)\n",
        "        domain_label = torch.ones(batch_size)\n",
        "        domain_label = domain_label.long()\n",
        "\n",
        "        if cuda:\n",
        "            t_img = t_img.cuda()\n",
        "            t_label = t_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(t_img).copy_(t_img)\n",
        "        class_label.resize_as_(t_label).copy_(t_label)\n",
        "\n",
        "\n",
        "        fea,class_output, domain_output = my_net(x=input_img, alpha=alpha)\n",
        "\n",
        "        cens=centerloss(class_label, fea)\n",
        "\n",
        "        err_t_label = loss_class(class_output, class_label)\n",
        "        err_t_domain = loss_domain(domain_output, domain_label)\n",
        "        err = err_t_domain + err_s_domain + 10*err_s_label+err_t_label*2+0.001*cens\n",
        "        # err=err_s_label\n",
        "        err.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print ('epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \\\n",
        "        #       % (epoch, i, len_dataloader, err_s_label.cpu().data.numpy(),\n",
        "        #          err_s_domain.cpu().data.numpy(), err_t_domain.cpu().data.numpy()))\n",
        "    \n",
        "        i=i+1\n",
        "\n",
        "    torch.save(my_net, '{0}/mnist_mnistm_model_epoch_{1}.pth'.format(model_root, epoch))\n",
        "traac=test('train',dataloader_source, epoch)\n",
        "targac=test('target',dataloader_target, epoch)\n",
        "tesac=test('test',test_loader, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rw7_lspcezHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DRDA_IV_2a.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}