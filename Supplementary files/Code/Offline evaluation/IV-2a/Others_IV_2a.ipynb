{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction"
      ],
      "metadata": {
        "id": "xqIqt8cJJSBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run each cell one by one**"
      ],
      "metadata": {
        "id": "Jd6OSQ0iJT7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start"
      ],
      "metadata": {
        "id": "gSCnTgMkJYFY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU1wHT_tnG4s"
      },
      "source": [
        "1. Load the dataset from the google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R1nShhmh_vPm"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pprint, pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "id_1_train=\"1GFFtOkPJskSjhA7-Klfj33fF5eVWa5sE\"\n",
        "id_1_evaluation=\"1G7cd67MlLEBn-OoY612T08yBoRoGyXyd\"\n",
        "\n",
        "\n",
        "id_2_train=\"1AP5V-adbxm-bdOuB-7EtgegqzqtlldxZ\"\n",
        "id_2_evaluation=\"1-XoFS7lPF9DQSO4ItK_KtorUaLFmA4Pn\"\n",
        "\n",
        "id_3_train=\"1RFuUrqJPqBWycC_j1b2Vy1DgBr1yQbPG\"\n",
        "id_3_evaluation=\"1fPSYHnTi5ORcI7QgXjIduAYOYZsoE9tI\"\n",
        "\n",
        "id_4_train=\"1jovz85ALrSRBZgLNUeeoihSJkEYJT3ZH\"\n",
        "id_4_evaluation=\"1e6C3vn7sPV-LO-Q1EbLTHvlcT7oqhJ99\"\n",
        "\n",
        "\n",
        "id_5_train=\"17wrRD332sGeRob6W36MNkQmj9Kc6u5lG\"\n",
        "id_5_evaluation=\"1JUc3flvygXmhdTL-kM_9pvh25pKfpvNy\"\n",
        "\n",
        "id_6_train=\"1vROntA0I75Gq85VXa9jwukl-IXxEOCCb\"\n",
        "id_6_evaluation=\"1pYnce5N2KxxNmFM2sgfxldLpaKgclEiY\"\n",
        "\n",
        "id_7_train=\"1_qvjVphyrE311iInuVDhhWWWN3BC4-sm\"\n",
        "id_7_evaluation=\"18V-Tp3Kny9aYz8rxSFsRQrYDtstVF75d\"\n",
        "\n",
        "\n",
        "id_8_train=\"1Cpi6yNpnMX5oCClCClv7Lcgg2RvxspUF\"\n",
        "id_8_evaluation=\"13cz0_VMYI0-u2LlEhmy61mGuzmjbmP1B\"\n",
        "\n",
        "id_9_train=\"1o0gyOZrYsyw68GFhpKFuwoFRn4pVwb7Q\"\n",
        "id_9_evaluation=\"1KW0_TzquGXygyBksatKEvevm6NYwddEA\"\n",
        "\n",
        "\n",
        "\n",
        "def load(id_train,id_evaluation,subject):\n",
        "  downloaded = drive.CreateFile({'id':id_train}) \n",
        "  downloaded.GetContentFile('A0'+str(subject)+'T.mat')\n",
        "  downloaded = drive.CreateFile({'id':id_evaluation}) \n",
        "  downloaded.GetContentFile('A0'+str(subject)+'E.mat')\n",
        "\n",
        "load(id_1_train,id_1_evaluation,1)\n",
        "load(id_2_train,id_2_evaluation,2)\n",
        "load(id_3_train,id_3_evaluation,3)\n",
        "\n",
        "load(id_4_train,id_4_evaluation,4)\n",
        "load(id_5_train,id_5_evaluation,5)\n",
        "load(id_6_train,id_6_evaluation,6)\n",
        "\n",
        "load(id_7_train,id_7_evaluation,7)\n",
        "load(id_8_train,id_8_evaluation,8)\n",
        "load(id_9_train,id_9_evaluation,9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GvrugCjnZfb"
      },
      "source": [
        "2. Load the package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-pAVSZXaoOqp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd.function import Function\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.utils.data as Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOeNys81qFsk"
      },
      "source": [
        "3. Convert the data in mat format into Numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ldAa6XJhA17Q"
      },
      "outputs": [],
      "source": [
        "def load_all_data (crossValidation, data_path): \n",
        "\n",
        "    big_X_train, big_y_train, big_X_test, big_y_test = [None]*9, [None]*9, [None]*9, [None]*9\n",
        "    for subject in range (0,9):\n",
        "        path = data_path+'s' + str(subject+1) + '/'\n",
        "        big_X_train[subject], big_y_train[subject] = get_data(subject+1, True ,path)\n",
        "        big_X_test[subject], big_y_test[subject] = get_data(subject+1, False ,path)\n",
        "    \n",
        "    return big_X_train, big_y_train, big_X_test, big_y_test\n",
        "\n",
        "def get_data_2(subject,training,path, highpass = False):\n",
        "\t'''\tLoads the dataset 2a of the BCI Competition IV\n",
        "\tavailable on http://bnci-horizon-2020.eu/database/data-sets\n",
        "\n",
        "\tKeyword arguments:\n",
        "\tsubject -- number of subject in [1, .. ,9]\n",
        "\ttraining -- if True, load training data\n",
        "\t\t\t\tif False, load testing data\n",
        "\t\n",
        "\tReturn:\tdata_return \tnumpy matrix \tsize = NO_valid_trial x 22 x 1750\n",
        "\t\t\tclass_return \tnumpy matrix \tsize = NO_valid_trial\n",
        "\t'''\n",
        "\tNO_channels = 22\n",
        "\tNO_tests = 6*48 \t\n",
        "\tWindow_Length = 7*250 \n",
        "\n",
        "\tclass_return = np.zeros(NO_tests)\n",
        "\tdata_return = np.zeros((NO_tests,NO_channels,Window_Length))\n",
        "\n",
        "\tNO_valid_trial = 0\n",
        "\tif training:\n",
        "\t\ta = sio.loadmat(path+'A0'+str(subject)+'T.mat')\n",
        "\telse:\n",
        "\t\ta = sio.loadmat(path+'A0'+str(subject)+'E.mat')\n",
        "\ta_data = a['data']\n",
        "\tfor ii in range(0,a_data.size):\n",
        "\t\ta_data1 = a_data[0,ii]\n",
        "\t\ta_data2= [a_data1[0,0]]\n",
        "\t\ta_data3= a_data2[0]\n",
        "\t\ta_X \t\t= a_data3[0]\n",
        "\t\ta_trial \t= a_data3[1]\n",
        "\t\ta_y \t\t= a_data3[2]\n",
        "\t\ta_fs \t\t= a_data3[3]\n",
        "\t\ta_classes \t= a_data3[4]\n",
        "\t\ta_artifacts = a_data3[5]\n",
        "\t\ta_gender \t= a_data3[6]\n",
        "\t\ta_age \t\t= a_data3[7]\n",
        "\n",
        "\t\tfor trial in range(0,a_trial.size):\n",
        "\t\t\tif(a_artifacts[trial]==0):\n",
        "\t\t\t\tdata_return[NO_valid_trial,:,:] = np.transpose(a_X[int(a_trial[trial]):(int(a_trial[trial])+Window_Length),:22])\n",
        "\t\t\t\tclass_return[NO_valid_trial] = int(a_y[trial])\n",
        "\t\t\t\tNO_valid_trial +=1\n",
        "\n",
        "\treturn data_return[0:NO_valid_trial,:,:], class_return[0:NO_valid_trial]\n",
        "\n",
        "def prepare_features(path,subject,crossValidation=False):\n",
        "    fs = 250 \n",
        "    # t1 = int(1.5*fs)\n",
        "    t1 = int(2*fs)\n",
        "\n",
        "    t2 = int(6*fs)\n",
        "    T = t2-t1\n",
        "    X_train, y_train = get_data_2(subject+1,True,path)\n",
        "    # X_train, y_train,train_artifact = get_data_2(subject+1,True,path)\n",
        "\n",
        "    if crossValidation:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=0)\n",
        "    else:\n",
        "        X_test, y_test = get_data_2(subject+1,False,path)\n",
        "\n",
        "    # prepare training data \t\n",
        "    N_tr,N_ch,_ =X_train.shape \n",
        "    X_train = X_train[:,:,t1:t2].reshape(N_tr,1,N_ch,T)\n",
        "    y_train_onehot = (y_train-1).astype(int)\n",
        "    y_train_onehot = to_categorical(y_train_onehot)\n",
        "    # prepare testing data \n",
        "    N_test,N_ch,_ =X_test.shape \n",
        "    X_test = X_test[:,:,t1:t2].reshape(N_test,1,N_ch,T)\n",
        "    y_test_onehot = (y_test-1).astype(int)\n",
        "    y_test_onehot = to_categorical(y_test_onehot)\t\n",
        "\n",
        "    return X_train,y_train,y_train_onehot,X_test,y_test,y_test_onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Wr8L0-x5-w"
      },
      "source": [
        "4. EEGNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1q37twcQEyeL"
      },
      "outputs": [],
      "source": [
        "class Constrainedlinear(nn.Linear):\n",
        "    def _max_norm(self,w,max_val=0.25,eps=1e-8):\n",
        "        norm = w.norm(2, dim=1, keepdim=True)\n",
        "        desired = torch.clamp(norm, 0, max_val)\n",
        "        return w * (desired / (eps + norm))\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self._max_norm(self.weight),self.bias)\n",
        "\n",
        "\n",
        "class Constrainedlinear111(nn.Linear):\n",
        "    def _max_norm(self,w,max_val=0.5,eps=1e-8):\n",
        "        norm = w.norm(2, dim=1, keepdim=True)\n",
        "        desired = torch.clamp(norm, 0, max_val)\n",
        "        return w * (desired / (eps + norm))\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self._max_norm(self.weight),self.bias)\n",
        "\n",
        "\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, classes_num):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.numC=22\n",
        "\n",
        "        self.drop_out = 0.2\n",
        "        \n",
        "        self.block_1 = nn.Sequential(\n",
        "            # Pads the input tensor boundaries with zero\n",
        "            # left, right, up, bottom\n",
        "            nn.ZeroPad2d((15, 16, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,          # input shape (1, C, T)\n",
        "                out_channels=8,         # num_filters\n",
        "                kernel_size=(1, 32),    # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (8, C, T)\n",
        "            nn.BatchNorm2d(8)           # output shape (8, C, T)\n",
        "        )\n",
        "        \n",
        "        # block 2 and 3 are implementations of Depthwise Convolution and Separable Convolution\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=8,          # input shape (8, C, T)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(self.numC, 1),    # filter size\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//4)\n",
        "            nn.Dropout(self.drop_out)   # output shape (16, 1, T//4)\n",
        "        )\n",
        "        \n",
        "        self.block_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "               in_channels=16,          # input shape (16, 1, T//4)\n",
        "               out_channels=16,         # num_filters\n",
        "               kernel_size=(1, 16),     # filter size\n",
        "               groups=16,\n",
        "               bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,         # input shape (16, 1, T//4)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(1, 1),     # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T//4)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//32)\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.preluip1 = nn.PReLU()\n",
        "        self.out = Constrainedlinear((16 * 15), classes_num)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.out(x)\n",
        "        return F.log_softmax(x, dim=1) #x   # return x for visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D-sxuh8yI6y"
      },
      "source": [
        "5. Load the raw data and assign the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CL-tE2VoFL_d"
      },
      "outputs": [],
      "source": [
        "def train(Subject_number):\n",
        "  #Prepare the raw feature data for training and testing#\n",
        "  #You can change the \"Subject_number\" (0-8 corresponding to B01-B09, respectively) to try training and testing between different subjects\n",
        "\n",
        "  BATCH_SIZE = 128\n",
        "  x_train,y_train,y_train_onehot,x_test,y_test,y_test_onehot = prepare_features('',Subject_number,False)\n",
        "  \n",
        "  total_x=np.vstack((x_train,x_test))\n",
        "  total_y=np.hstack((y_train,y_test))\n",
        "  index1=np.where(total_y == 1)[0]\n",
        "  index2=np.where(total_y == 2)[0]\n",
        "  index3=np.where(total_y == 3)[0]\n",
        "  index4=np.where(total_y == 4)[0]\n",
        "\n",
        "  choice1=np.random.choice(index1,15,replace=False)\n",
        "  choice2=np.random.choice(index2,15,replace=False)\n",
        "  choice3=np.random.choice(index3,15,replace=False)\n",
        "  choice4=np.random.choice(index4,15,replace=False)\n",
        "\n",
        "\n",
        "  choice_total=np.hstack((choice1,choice2,choice3,choice4))\n",
        "  xxxtest= np.delete(total_x, choice_total,0)\n",
        "  yyytest=np.delete(total_y, choice_total)\n",
        "\n",
        "\n",
        "  total_x_train=[]\n",
        "  total_y_train=[]\n",
        "  full_subjectnum=[0,1,2,3,4,5,6,7,8]\n",
        "  full_subjectnum.remove(Subject_number)\n",
        "  for train_subject in full_subjectnum:\n",
        "    x_train,y_train,y_train_onehot,x_test,y_test,y_test_onehot = prepare_features('',train_subject,False)\n",
        "\n",
        "    if len(total_x_train)==0:\n",
        "      total_x_train=x_train\n",
        "      total_y_train=y_train\n",
        "    else:\n",
        "      total_x_train=np.vstack((total_x_train,x_train))\n",
        "      total_y_train=np.hstack((total_y_train,y_train))\n",
        "    total_x_train=np.vstack((total_x_train,x_test))\n",
        "    total_y_train=np.hstack((total_y_train,y_test))\n",
        "  x_train=total_x_train\n",
        "  y_train=total_y_train\n",
        "\n",
        "\n",
        "  x_test=xxxtest\n",
        "  y_test=yyytest\n",
        "\n",
        "\n",
        "  x_train=torch.from_numpy(x_train)\n",
        "  y_train=torch.from_numpy(y_train-1)\n",
        "  x_test=torch.from_numpy(x_test)\n",
        "  y_test=torch.from_numpy(y_test-1)\n",
        "  x_train=x_train.float()\n",
        "  x_test=x_test.float()\n",
        "  y_train=y_train.long()\n",
        "  y_test=y_test.long()\n",
        "  train_dataset=Data.TensorDataset(x_train,y_train)\n",
        "  val_dataset=Data.TensorDataset(x_test,y_test)\n",
        "\n",
        "  #Construct the training Loader\n",
        "  train_loader= Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "  test_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  eeg_net = EEGNet(classes_num=4).to(device)\n",
        "\n",
        "  # summary(eeg_net,(1, 3, 1000))\n",
        "\n",
        "  nllloss = nn.NLLLoss().to(device) #CrossEntropyLoss = log_softmax + NLLLoss\n",
        "\n",
        "# optimzer4nn\n",
        "  optimizer4nn = optim.Adam(eeg_net.parameters(),lr=0.001)\n",
        "\n",
        "  #Define the number of epochs\n",
        "  EPOCH=500\n",
        "\n",
        "#Define the device\n",
        "  DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  for epoch in range(EPOCH):\n",
        "    loss_total=[]\n",
        "\n",
        "  #Update x_i using the gradient descent\n",
        "    for step, (b_x, b_y) in enumerate(train_loader):\n",
        "      b_x, b_y = b_x.to(DEVICE), b_y.to(DEVICE)\n",
        "      output = eeg_net(b_x)\n",
        "      loss = nllloss(output,b_y) \n",
        "      loss_total.append(loss.item())\n",
        "      optimizer4nn.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer4nn.step()\n",
        "    if (epoch+1)%100==0:\n",
        "      print('Epoch: ', epoch+1, '| train loss: %.4f' % (np.mean(loss_total)),len(loss_total))\n",
        " \n",
        "  eeg_net.eval()\n",
        "  train_out=eeg_net(x_train.to(DEVICE))\n",
        "  trpre_y = torch.max(train_out.cpu(), 1)[1].data.numpy()\n",
        "  tra_acc = float((trpre_y == y_train.data.numpy()).astype(int).sum()) / float(y_train.size(0))\n",
        "  print(\"Subject \"+str(Subject_number+1)+' train accuracy: %.4f' % tra_acc)\n",
        "\n",
        "\n",
        "  test_out = eeg_net(x_test.to(DEVICE))\n",
        "  pred_y = torch.max(test_out.cpu(), 1)[1].data.numpy()\n",
        "  accuracy = float((pred_y == y_test.data.numpy()).astype(int).sum()) / float(y_test.size(0))\n",
        "  print(\"Subject \"+str(Subject_number+1)+' test accuracy: %.4f' % accuracy)\n",
        "  print('')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATmkrttS2d1D"
      },
      "source": [
        "6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7oG8ym7F1RF",
        "outputId": "b84a23ae-000c-4d57-9171-d06a5bc4cc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject 0\n",
            "Epoch:  100 | train loss: 0.7553 33\n",
            "Epoch:  200 | train loss: 0.6561 33\n",
            "Epoch:  300 | train loss: 0.5993 33\n",
            "Epoch:  400 | train loss: 0.5705 33\n",
            "Epoch:  500 | train loss: 0.5594 33\n",
            "Subject 1 train accuracy: 0.8535\n",
            "Subject 1 test accuracy: 0.6822\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Change the subject number 0-8 corrsponding to subject A01-A09Subject_number=0\n",
        "Subject_number=0\n",
        "print(\"Subject\",Subject_number)\n",
        "train(Subject_number)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Others_IV_2a.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}