{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction"
      ],
      "metadata": {
        "id": "nOJo-n0pKrHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run each cell one by one**"
      ],
      "metadata": {
        "id": "au1_W3DAKrKK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYCCkAM20MYW"
      },
      "source": [
        "# Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ruyr8-MtlIB7"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pprint, pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id_1_train=\"13N5Il4V77Od7rK5ywff7YBWDD9PGJBdT\"\n",
        "id_1_evaluation=\"1RFAezNqq00h8lYs1f_kywJ0lDLhx-sKG\"\n",
        "\n",
        "id_2_train=\"17YZy5p42Bprj2Bo19lpH04iIBgf8J42l\"\n",
        "id_2_evaluation=\"13ZuGeuCSopUsPDMUz5TRZfaSuGnA_kZ7\"\n",
        "\n",
        "id_3_train=\"1mRJkhas7St256PhJPUI4TDQN2ENA2RnO\"\n",
        "id_3_evaluation=\"1fyk1WmHIk3V_XCDUch28DwWhHXFitogM\"\n",
        "\n",
        "id_4_train=\"1uEn_rGZMGLEcYuIfrXBcYroAOKTvkVRw\"\n",
        "id_4_evaluation=\"1jnp1LOgUcZhzcB7weaEQ3FTGk2gNGHcc\"\n",
        "\n",
        "\n",
        "id_5_train=\"1AGmGhS6pH5ED5v1Qc8RyJbHdybKG4G96\"\n",
        "id_5_evaluation=\"1IC74vwV5_IyBvBFfSpqIACzgYOI-2i4u\"\n",
        "\n",
        "id_6_train=\"1Td_zmiqkpKNQBwBje7pyJoRde9wUjABh\"\n",
        "id_6_evaluation=\"1XiJcjN1-Pe7-R89PugtFspWilF3QOlqi\"\n",
        "\n",
        "id_7_train=\"1dujTQPIGDBSozlbXt_hJj_qKIftxWVka\"\n",
        "id_7_evaluation=\"1LniXihHiqx1PSlvmJXu20LaOeXCjBmeQ\"\n",
        "\n",
        "\n",
        "id_8_train=\"1T66t4HpF1XTB6QNL9ZdwILWGU5sEmf09\"\n",
        "id_8_evaluation=\"1lxZVBtjtdzF7aZPx4oh2265m-049XgnO\"\n",
        "\n",
        "id_9_train=\"1zGIpgC3RNRb1qF8t-id3IGtvd92C7IQV\"\n",
        "id_9_evaluation=\"1guGz6wBBwzTrRNQGCvP-ar2GuI8ZQWe7\"\n",
        "\n",
        "def load(id_train,id_evaluation,subject):\n",
        "  downloaded = drive.CreateFile({'id':id_train}) \n",
        "  downloaded.GetContentFile('B0'+str(subject)+'T.mat')\n",
        "  downloaded = drive.CreateFile({'id':id_evaluation}) \n",
        "  downloaded.GetContentFile('B0'+str(subject)+'E.mat')\n",
        "\n",
        "load(id_1_train,id_1_evaluation,1)\n",
        "load(id_2_train,id_2_evaluation,2)\n",
        "load(id_3_train,id_3_evaluation,3)\n",
        "\n",
        "load(id_4_train,id_4_evaluation,4)\n",
        "load(id_5_train,id_5_evaluation,5)\n",
        "load(id_6_train,id_6_evaluation,6)\n",
        "\n",
        "load(id_7_train,id_7_evaluation,7)\n",
        "load(id_8_train,id_8_evaluation,8)\n",
        "load(id_9_train,id_9_evaluation,9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrAnxBIalLoc"
      },
      "source": [
        "# Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8bvfxtJilOo-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd.function import Function\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.utils.data as Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atSHs5e7lRuW"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YRCtA1s0lTfR"
      },
      "outputs": [],
      "source": [
        "def load_all_data (crossValidation, data_path): \n",
        "\n",
        "    big_X_train, big_y_train, big_X_test, big_y_test = [None]*9, [None]*9, [None]*9, [None]*9\n",
        "    for subject in range (0,9):\n",
        "        path = data_path+'s' + str(subject+1) + '/'\n",
        "        big_X_train[subject], big_y_train[subject] = get_data(subject+1, True ,path)\n",
        "        big_X_test[subject], big_y_test[subject] = get_data(subject+1, False ,path)\n",
        "    \n",
        "    return big_X_train, big_y_train, big_X_test, big_y_test\n",
        "\n",
        "def get_data(subject,training,path, highpass = False):\n",
        "\t'''\tLoads the dataset 2a of the BCI Competition IV\n",
        "\tavailable on http://bnci-horizon-2020.eu/database/data-sets\n",
        "\n",
        "\tKeyword arguments:\n",
        "\tsubject -- number of subject in [1, .. ,9]\n",
        "\ttraining -- if True, load training data\n",
        "\t\t\t\tif False, load testing data\n",
        "\t\n",
        "\tReturn:\tdata_return \tnumpy matrix \tsize = NO_valid_trial x 22 x 1750\n",
        "\t\t\tclass_return \tnumpy matrix \tsize = NO_valid_trial\n",
        "\t'''\n",
        "\tNO_channels = 3\n",
        "\tNO_tests = 120+120+160\t\n",
        "\tWindow_Length = 7*250 \n",
        "\n",
        "\tclass_return = np.zeros(NO_tests)\n",
        "\tdata_return = np.zeros((NO_tests,NO_channels,Window_Length))\n",
        "\n",
        "\tNO_valid_trial = 0\n",
        "\tif training:\n",
        "\t\ta = sio.loadmat(path+'B0'+str(subject)+'T.mat')\n",
        "\telse:\n",
        "\t\ta = sio.loadmat(path+'B0'+str(subject)+'E.mat')\n",
        "\ta_data = a['data']\n",
        "\tfor ii in range(0,a_data.size):\n",
        "\t\ta_data1 = a_data[0,ii]\n",
        "\t\ta_data2= [a_data1[0,0]]\n",
        "\t\ta_data3= a_data2[0]\n",
        "\t\ta_X \t\t= a_data3[0]\n",
        "\t\ta_trial \t= a_data3[1]\n",
        "\t\ta_y \t\t= a_data3[2]\n",
        "\t\ta_fs \t\t= a_data3[3]\n",
        "\t\ta_classes \t= a_data3[4]\n",
        "\t\ta_artifacts = a_data3[5]\n",
        "\t\ta_gender \t= a_data3[6]\n",
        "\t\ta_age \t\t= a_data3[7]\n",
        "\n",
        "\t\tfor trial in range(0,a_trial.size):\n",
        "\t\t\tif(a_artifacts[trial]==0):\n",
        "\t\t\t\tdata_return[NO_valid_trial,:,:] = np.transpose(a_X[int(a_trial[trial]):(int(a_trial[trial])+Window_Length),:3])\n",
        "\t\t\t\tclass_return[NO_valid_trial] = int(a_y[trial])\n",
        "\t\t\t\tNO_valid_trial +=1\n",
        "\n",
        "\n",
        "\treturn data_return[0:NO_valid_trial,:,:], class_return[0:NO_valid_trial]\n",
        "\n",
        "def prepare_features(path,subject,crossValidation=False):\n",
        "    fs = 250 \n",
        "    t1 = int(3*fs)\n",
        "    t2 = int(7*fs)\n",
        "    T = t2-t1\n",
        "    X_train, y_train = get_data(subject+1,True,path)\n",
        "    if crossValidation:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=0)\n",
        "    else:\n",
        "        X_test, y_test = get_data(subject+1,False,path)\n",
        "\n",
        "    # prepare training data \t\n",
        "    N_tr,N_ch,_ =X_train.shape \n",
        "    X_train = X_train[:,:,t1:t2].reshape(N_tr,1,N_ch,T)\n",
        "    y_train_onehot = (y_train-1).astype(int)\n",
        "    y_train_onehot = to_categorical(y_train_onehot)\n",
        "    # prepare testing data \n",
        "    N_test,N_ch,_ =X_test.shape \n",
        "    X_test = X_test[:,:,t1:t2].reshape(N_test,1,N_ch,T)\n",
        "    y_test_onehot = (y_test-1).astype(int)\n",
        "    y_test_onehot = to_categorical(y_test_onehot)\t\n",
        "    return X_train,y_train,y_train_onehot,X_test,y_test,y_test_onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3--hc7U1kcY"
      },
      "source": [
        "# Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xju6ME7i1cFL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "hyperparameter_se=10\n",
        "\n",
        "\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, classes_num):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.numC=classes_num\n",
        "\n",
        "        self.drop_out = 0\n",
        "        \n",
        "        self.block_1 = nn.Sequential(\n",
        "            # Pads the input tensor boundaries with zero\n",
        "            # left, right, up, bottom\n",
        "            nn.ZeroPad2d((15, 16, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,          # input shape (1, C, T)\n",
        "                out_channels=8,         # num_filters\n",
        "                kernel_size=(1, 32),    # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (8, C, T)\n",
        "            nn.BatchNorm2d(8)           # output shape (8, C, T)\n",
        "        )\n",
        "        \n",
        "        # block 2 and 3 are implementations of Depthwise Convolution and Separable Convolution\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=8,          # input shape (8, C, T)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(self.numC, 1),    # filter size\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//4)\n",
        "            nn.Dropout(self.drop_out)   # output shape (16, 1, T//4)\n",
        "        )\n",
        "        \n",
        "        self.block_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "               in_channels=16,          # input shape (16, 1, T//4)\n",
        "               out_channels=16,         # num_filters\n",
        "               kernel_size=(1, 16),     # filter size\n",
        "               groups=16,\n",
        "               bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,         # input shape (16, 1, T//4)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(1, 1),     # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T//4)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//32)\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x \n",
        "\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RelationNetwork(nn.Module):\n",
        "    \"\"\"docstring for RelationNetwork\"\"\"\n",
        "    def __init__(self,input_size,hidden_size):\n",
        "        super(RelationNetwork, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(30,16,kernel_size=(1,2),padding=1),\n",
        "                        nn.BatchNorm2d(16, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(16,16,kernel_size=(1,2),padding=1),\n",
        "                        nn.BatchNorm2d(16, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(64,hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=x.view(-1,30,1,16)\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        \n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = torch.sigmoid(self.fc2(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "kersize=5\n",
        "class Time_net(nn.Module):\n",
        "  # def __init__(self, feature_dim,hidden_dim, n_layers):\n",
        "  def __init__(self,train_num):\n",
        "\n",
        "        super(Time_net, self).__init__()\n",
        "        # self.n_layers = n_layers\n",
        "        # self.hidden_dim = hidden_dim\n",
        "        # self.lstm = nn.RNN(feature_dim, hidden_dim, n_layers)\n",
        "        self.pad = nn.ZeroPad2d((0, 0, kersize-1, 0))\n",
        "        self.series_decoding= nn.Conv2d(1,1,kernel_size=(kersize,1))#,padding='same')#(hyperparameter_se,0))\n",
        "      \n",
        "  def forward(self, x):\n",
        "        x = x.view(1,x.size(0),x.size(1))\n",
        "        x = self.pad(x)\n",
        "        x =  self.series_decoding(x)\n",
        "        # # print(x.shape)\n",
        "        # lstm_out, hidden = self.lstm(x)\n",
        "        x = x.view(x.size(1),-1)\n",
        "        # return lstm_out\n",
        "        return x\n",
        "\n",
        "\n",
        "# class RelationNetwork(nn.Module):\n",
        "#     \"\"\"docstring for RelationNetwork\"\"\"\n",
        "#     def __init__(self,hidden_dim,middle_dim):\n",
        "#         super(RelationNetwork, self).__init__()\n",
        "#         self.layer1 = nn.Linear(hidden_dim,middle_dim)\n",
        "#         self.layer2 = nn.Linear(middle_dim,1)\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         out = F.relu(self.layer1(x))\n",
        "#         out = torch.sigmoid(self.layer2(out))\n",
        "#         return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLUS-CZXsCEH"
      },
      "source": [
        "# Data generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_set(label_set,num):\n",
        "  index1=np.where(label_set == 0)[0]\n",
        "  index2=np.where(label_set == 1)[0]\n",
        "  number=max(index1[num-1],index2[num-1])+1\n",
        "  return number\n",
        "  \n"
      ],
      "metadata": {
        "id": "lP-RULt-rZNh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "E9lTuzqyQpaw"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "class BCI_task(object):\n",
        "\n",
        "    def __init__(self, feature_list,label,train_num,test_num,settype):\n",
        "\n",
        "        self.train_num = train_num\n",
        "        self.test_num = test_num\n",
        "        if settype==\"test\":\n",
        "          testmode=mini_set(label,train_num)\n",
        "          # testmode=40\n",
        "          supportset=label[:testmode]\n",
        "          index1=np.where(supportset == 0)[0]\n",
        "          index2=np.where(supportset == 1)[0]\n",
        "          choice1=np.random.choice(index1,train_num,replace=False)\n",
        "          choice2=np.random.choice(index2,train_num,replace=False)\n",
        "          choice_total=np.sort(np.hstack((choice1,choice2)))\n",
        "          self.training_feature = feature_list[choice_total]\n",
        "          self.train_labels= label[choice_total]\n",
        "          test_list=list(range(testmode))\n",
        "          self.testing_feature= np.delete(feature_list, test_list,0)\n",
        "          self.test_labels=np.delete(label, test_list)\n",
        "\n",
        "\n",
        "        else:\n",
        "          index1=np.where(label == 0)[0]\n",
        "          index2=np.where(label == 1)[0]\n",
        "          choice1=np.random.choice(index1,train_num+test_num,replace=False)\n",
        "          choice2=np.random.choice(index2,train_num+test_num,replace=False)\n",
        "          train_choice=np.sort(np.hstack((choice1[:train_num],choice2[:train_num])))\n",
        "          test_choice=np.hstack((choice1[train_num:],choice2[train_num:]))\n",
        "\n",
        "          self.training_feature = feature_list[train_choice]\n",
        "          self.train_labels= label[train_choice]\n",
        "          self.testing_feature = feature_list[test_choice]\n",
        "          self.test_labels =label[test_choice]\n",
        "\n",
        "\n",
        "\n",
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, task, split='train'):\n",
        "        self.task = task\n",
        "        self.split = split\n",
        "        self.feature = self.task.training_feature if self.split == 'train' else self.task.testing_feature\n",
        "        self.labels = self.task.train_labels if self.split == 'train' else self.task.test_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raise NotImplementedError(\"This is an abstract class. Subclass this class for your particular dataset.\")\n",
        "\n",
        "\n",
        "class BCI_DATA(FewShotDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(BCI_DATA, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.feature[idx]\n",
        "        label = self.labels[idx]\n",
        "        return feature.astype(np.float32), label\n",
        "\n",
        "\n",
        "def get_data_loader(task, split='train',shuffle=False):\n",
        "    dataset = BCI_DATA(task,split=split)\n",
        "    number = len(task.training_feature) if split == 'train' else len(task.testing_feature)\n",
        "    loader = DataLoader(dataset, batch_size=number,shuffle=shuffle)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpzkQE4pa840"
      },
      "source": [
        "# Weight initializer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-ytN9D43a_3Q"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn.init as init\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        n = m.weight.size(1)\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data = torch.ones(m.bias.data.size())\n",
        "\n",
        "def wieht(m):\n",
        "    if isinstance(m, nn.LSTM):\n",
        "      for param in m.parameters():\n",
        "        init.normal_(param.data,mean=0.1,std=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nCxxk1el1Vl"
      },
      "source": [
        "# Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KEw-s9R4l3IM"
      },
      "outputs": [],
      "source": [
        "TRAIN_NUM=hyperparameter_se\n",
        "TEST_NUM=hyperparameter_se\n",
        "LEARNING_RATE=0.001\n",
        "\n",
        "EPSIODE_NUM=15000\n",
        "\n",
        "\n",
        "FEATURE_DIM=240\n",
        "RELATION_DIM=240\n",
        "n_layers=1\n",
        "MIDDLE_DIM=16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UND1ZLK4c_f2"
      },
      "source": [
        "\n",
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9Cvqxn_mKXVp"
      },
      "outputs": [],
      "source": [
        "dic=[]\n",
        "\n",
        "for x in range(9):\n",
        "  temp=[]\n",
        "  x_train,y_train,y_train_onehot,x_test,y_test,y_test_onehot = prepare_features('',x,False)\n",
        "  xxxtest=np.vstack((x_train,x_test))\n",
        "  yyytest=np.hstack((y_train,y_test))-1\n",
        "  temp.append(xxxtest)\n",
        "  temp.append(yyytest)\n",
        "\n",
        "  dic.append(temp)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xVeqn3Cnz0b7"
      },
      "outputs": [],
      "source": [
        "def main(Subject_number):\n",
        "\n",
        "  # Seed\n",
        "  GPU = 1\n",
        "  torch.manual_seed(GPU)\n",
        "  torch.cuda.manual_seed(GPU)\n",
        "  torch.cuda.manual_seed_all(GPU)\n",
        "\n",
        "  np.random.seed(GPU)\n",
        "  random.seed(GPU)\n",
        "\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "  feature_encoder = EEGNet(classes_num=3)\n",
        "  Temporal_CNN=Time_net(TRAIN_NUM)\n",
        "  relation_network = RelationNetwork(RELATION_DIM*2,MIDDLE_DIM)\n",
        "\n",
        "\n",
        "  feature_encoder.apply(weights_init)\n",
        "  relation_network.apply(weights_init)\n",
        "  GPU = 0\n",
        "\n",
        "\n",
        "  feature_encoder.cuda(GPU)\n",
        "  Temporal_CNN.cuda(GPU)\n",
        "  relation_network.cuda(GPU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  full_subjectnum=[0,1,2,3,4,5,6,7,8]\n",
        "  full_subjectnum.remove(Subject_number)\n",
        "  total_losss=[]\n",
        "\n",
        "  feature_encoder_optim = torch.optim.Adam(feature_encoder.parameters(),lr=LEARNING_RATE)\n",
        "  Temporal_CNN_optim = torch.optim.Adam(Temporal_CNN.parameters(),lr=LEARNING_RATE)\n",
        "  relation_network_optim = torch.optim.Adam(relation_network.parameters(),lr=LEARNING_RATE)\n",
        "  testlossset=[]\n",
        "  for episode in range(EPSIODE_NUM):\n",
        "    select_subject=np.random.choice(full_subjectnum, 1)[0]\n",
        "    x_train,y_train = dic[select_subject][0],dic[select_subject][1]\n",
        "\n",
        "    bci_task=BCI_task(x_train,y_train,train_num=TRAIN_NUM,test_num=TEST_NUM,settype='train')\n",
        "\n",
        "    sample_dataloader = get_data_loader(bci_task,split=\"train\",shuffle=False)\n",
        "    batch_dataloader = get_data_loader(bci_task,split=\"test\",shuffle=False)\n",
        "\n",
        "    samples,sample_labels = sample_dataloader.__iter__().next()\n",
        "    batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "    sample_labels=sample_labels.type(torch.int32).cuda(GPU)\n",
        "    sample_features = feature_encoder(Variable(samples).cuda(GPU))\n",
        "\n",
        "    sample_features=Temporal_CNN(sample_features)\n",
        "\n",
        "    nrow = torch.unique(sample_labels).size(0)\n",
        "    out = torch.zeros((nrow, sample_features.size(1)), dtype=sample_features.dtype).cuda(GPU)\n",
        "    out.index_add_(0, sample_labels, sample_features)\n",
        "    sample_features=out    \n",
        "    batch_features = feature_encoder(Variable(batches).cuda(GPU))\n",
        "\n",
        "    NUMEUSED=TEST_NUM*2\n",
        "    sample_features_ext = sample_features.unsqueeze(0).repeat(NUMEUSED,1,1)\n",
        "    batch_features_ext = batch_features.unsqueeze(0).repeat(sample_features.size(0),1,1)\n",
        "    batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "\n",
        "\n",
        "\n",
        "    relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,batch_features_ext.size()[2]*2)\n",
        "    relations = relation_network(relation_pairs)\n",
        "    relations = relations.view(NUMEUSED,-1)\n",
        "    mse = nn.MSELoss(reduction='mean').cuda(GPU)\n",
        "    one_hot_labels =F.one_hot(batch_labels.to(torch.int64), num_classes=2).float().cuda(GPU)\n",
        "\n",
        "    loss = mse(relations,one_hot_labels)\n",
        "\n",
        "    feature_encoder.zero_grad()\n",
        "    Temporal_CNN.zero_grad()\n",
        "    relation_network.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(feature_encoder.parameters(),0.5)\n",
        "    torch.nn.utils.clip_grad_norm_(Temporal_CNN.parameters(),0.5)\n",
        "    torch.nn.utils.clip_grad_norm_(relation_network.parameters(),0.5)\n",
        "\n",
        "    feature_encoder_optim.step()\n",
        "    Temporal_CNN_optim.step()\n",
        "    relation_network_optim.step()\n",
        "\n",
        "\n",
        "    total_losss.append(loss.item())\n",
        "    if (episode+1)%1000 == 0:\n",
        "        total_losss=[]\n",
        "        x_test,y_test = dic[Subject_number][0],dic[Subject_number][1]\n",
        "\n",
        "        bci_task=BCI_task(x_test,y_test,train_num=TRAIN_NUM,test_num=TEST_NUM,settype='test')\n",
        "        sample_dataloader = get_data_loader(bci_task,split=\"train\",shuffle=False)\n",
        "        batch_dataloader = get_data_loader(bci_task,split=\"test\",shuffle=False)\n",
        "\n",
        "        samples,sample_labels = sample_dataloader.__iter__().next()\n",
        "        batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "        sample_labels=sample_labels.type(torch.int32).cuda(GPU)\n",
        "        sample_features = feature_encoder(Variable(samples).cuda(GPU))\n",
        "\n",
        "        sample_features=Temporal_CNN(sample_features)\n",
        "\n",
        "        nrow = torch.unique(sample_labels).size(0)\n",
        "        out = torch.zeros((nrow, sample_features.size(1)), dtype=sample_features.dtype).cuda(GPU)\n",
        "        out.index_add_(0, sample_labels, sample_features)\n",
        "        sample_features=out    \n",
        "        batch_features = feature_encoder(Variable(batches).cuda(GPU))\n",
        "        \n",
        "\n",
        "        sample_features_ext = sample_features.unsqueeze(0).repeat(len(bci_task.testing_feature),1,1)\n",
        "        batch_features_ext = batch_features.unsqueeze(0).repeat(sample_features.size(0),1,1)\n",
        "        batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "\n",
        "        \n",
        "        relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,batch_features_ext.size()[2]*2)\n",
        "        relations = relation_network(relation_pairs)\n",
        "        relations = relations.view(len(bci_task.testing_feature),-1)\n",
        "        _,predict_labels = torch.max(relations.data,1)\n",
        "        rewards = [1 if predict_labels[j]==batch_labels[j] else 0 for j in range(len(bci_task.testing_feature))]\n",
        "        one_hot_labels =F.one_hot(batch_labels.to(torch.int64), num_classes=2).float().cuda(GPU)\n",
        "        testing_loss = mse(relations,one_hot_labels)\n",
        "        \n",
        "        testlossset.append(np.sum(rewards)/len(rewards)*100)\n",
        "\n",
        "  print(\"Last episode test accuracy:\",np.sum(rewards)/len(rewards)*100,\"%\")\n",
        "  print(\"Highest three each 1000 episodes:\",np.sort(testlossset)[-3:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkUnRPNnvR7q"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLJcTOXZlo3a",
        "outputId": "9a4acefc-4ebb-415f-cd60-d560a6b4dc2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last episode test accuracy: 93.73177843 %\n",
            "Highest three each 1000 episodes: [93.58600583 93.73177843 95.04373178]\n"
          ]
        }
      ],
      "source": [
        "# Change the subject number 0-8 corrsponding to subject B01-B09; Subject_number=0\n",
        "\n",
        "subject_number=3\n",
        "main(subject_number)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model_A_IV_2b.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}