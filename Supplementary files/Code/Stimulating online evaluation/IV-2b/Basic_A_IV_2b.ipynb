{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_A_IV_2b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction"
      ],
      "metadata": {
        "id": "JyWesU_ZM1WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run each cell one by one**"
      ],
      "metadata": {
        "id": "tdgE4GqVM1wM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n"
      ],
      "metadata": {
        "id": "JYCCkAM20MYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pprint, pickle\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id_1_train=\"13N5Il4V77Od7rK5ywff7YBWDD9PGJBdT\"\n",
        "id_1_evaluation=\"1RFAezNqq00h8lYs1f_kywJ0lDLhx-sKG\"\n",
        "\n",
        "id_2_train=\"17YZy5p42Bprj2Bo19lpH04iIBgf8J42l\"\n",
        "id_2_evaluation=\"13ZuGeuCSopUsPDMUz5TRZfaSuGnA_kZ7\"\n",
        "\n",
        "id_3_train=\"1mRJkhas7St256PhJPUI4TDQN2ENA2RnO\"\n",
        "id_3_evaluation=\"1fyk1WmHIk3V_XCDUch28DwWhHXFitogM\"\n",
        "\n",
        "id_4_train=\"1uEn_rGZMGLEcYuIfrXBcYroAOKTvkVRw\"\n",
        "id_4_evaluation=\"1jnp1LOgUcZhzcB7weaEQ3FTGk2gNGHcc\"\n",
        "\n",
        "\n",
        "id_5_train=\"1AGmGhS6pH5ED5v1Qc8RyJbHdybKG4G96\"\n",
        "id_5_evaluation=\"1IC74vwV5_IyBvBFfSpqIACzgYOI-2i4u\"\n",
        "\n",
        "id_6_train=\"1Td_zmiqkpKNQBwBje7pyJoRde9wUjABh\"\n",
        "id_6_evaluation=\"1XiJcjN1-Pe7-R89PugtFspWilF3QOlqi\"\n",
        "\n",
        "id_7_train=\"1dujTQPIGDBSozlbXt_hJj_qKIftxWVka\"\n",
        "id_7_evaluation=\"1LniXihHiqx1PSlvmJXu20LaOeXCjBmeQ\"\n",
        "\n",
        "\n",
        "id_8_train=\"1T66t4HpF1XTB6QNL9ZdwILWGU5sEmf09\"\n",
        "id_8_evaluation=\"1lxZVBtjtdzF7aZPx4oh2265m-049XgnO\"\n",
        "\n",
        "id_9_train=\"1zGIpgC3RNRb1qF8t-id3IGtvd92C7IQV\"\n",
        "id_9_evaluation=\"1guGz6wBBwzTrRNQGCvP-ar2GuI8ZQWe7\"\n",
        "\n",
        "def load(id_train,id_evaluation,subject):\n",
        "  downloaded = drive.CreateFile({'id':id_train}) \n",
        "  downloaded.GetContentFile('B0'+str(subject)+'T.mat')\n",
        "  downloaded = drive.CreateFile({'id':id_evaluation}) \n",
        "  downloaded.GetContentFile('B0'+str(subject)+'E.mat')\n",
        "\n",
        "load(id_1_train,id_1_evaluation,1)\n",
        "load(id_2_train,id_2_evaluation,2)\n",
        "load(id_3_train,id_3_evaluation,3)\n",
        "\n",
        "load(id_4_train,id_4_evaluation,4)\n",
        "load(id_5_train,id_5_evaluation,5)\n",
        "load(id_6_train,id_6_evaluation,6)\n",
        "\n",
        "load(id_7_train,id_7_evaluation,7)\n",
        "load(id_8_train,id_8_evaluation,8)\n",
        "load(id_9_train,id_9_evaluation,9)"
      ],
      "metadata": {
        "id": "Ruyr8-MtlIB7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package"
      ],
      "metadata": {
        "id": "BrAnxBIalLoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd.function import Function\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.utils.data as Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n"
      ],
      "metadata": {
        "id": "8bvfxtJilOo-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "atSHs5e7lRuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_data (crossValidation, data_path): \n",
        "\n",
        "    big_X_train, big_y_train, big_X_test, big_y_test = [None]*9, [None]*9, [None]*9, [None]*9\n",
        "    for subject in range (0,9):\n",
        "        path = data_path+'s' + str(subject+1) + '/'\n",
        "        big_X_train[subject], big_y_train[subject] = get_data(subject+1, True ,path)\n",
        "        big_X_test[subject], big_y_test[subject] = get_data(subject+1, False ,path)\n",
        "    \n",
        "    return big_X_train, big_y_train, big_X_test, big_y_test\n",
        "\n",
        "def get_data(subject,training,path, highpass = False):\n",
        "\t'''\tLoads the dataset 2a of the BCI Competition IV\n",
        "\tavailable on http://bnci-horizon-2020.eu/database/data-sets\n",
        "\n",
        "\tKeyword arguments:\n",
        "\tsubject -- number of subject in [1, .. ,9]\n",
        "\ttraining -- if True, load training data\n",
        "\t\t\t\tif False, load testing data\n",
        "\t\n",
        "\tReturn:\tdata_return \tnumpy matrix \tsize = NO_valid_trial x 22 x 1750\n",
        "\t\t\tclass_return \tnumpy matrix \tsize = NO_valid_trial\n",
        "\t'''\n",
        "\tNO_channels = 3\n",
        "\tNO_tests = 120+120+160\t\n",
        "\tWindow_Length = 7*250 \n",
        "\n",
        "\tclass_return = np.zeros(NO_tests)\n",
        "\tdata_return = np.zeros((NO_tests,NO_channels,Window_Length))\n",
        "\n",
        "\tNO_valid_trial = 0\n",
        "\tif training:\n",
        "\t\ta = sio.loadmat(path+'B0'+str(subject)+'T.mat')\n",
        "\telse:\n",
        "\t\ta = sio.loadmat(path+'B0'+str(subject)+'E.mat')\n",
        "\ta_data = a['data']\n",
        "\tfor ii in range(0,a_data.size):\n",
        "\t\ta_data1 = a_data[0,ii]\n",
        "\t\ta_data2= [a_data1[0,0]]\n",
        "\t\ta_data3= a_data2[0]\n",
        "\t\ta_X \t\t= a_data3[0]\n",
        "\t\ta_trial \t= a_data3[1]\n",
        "\t\ta_y \t\t= a_data3[2]\n",
        "\t\ta_fs \t\t= a_data3[3]\n",
        "\t\ta_classes \t= a_data3[4]\n",
        "\t\ta_artifacts = a_data3[5]\n",
        "\t\ta_gender \t= a_data3[6]\n",
        "\t\ta_age \t\t= a_data3[7]\n",
        "\n",
        "\t\tfor trial in range(0,a_trial.size):\n",
        "\t\t\tif(a_artifacts[trial]==0):\n",
        "\t\t\t\tdata_return[NO_valid_trial,:,:] = np.transpose(a_X[int(a_trial[trial]):(int(a_trial[trial])+Window_Length),:3])\n",
        "\t\t\t\tclass_return[NO_valid_trial] = int(a_y[trial])\n",
        "\t\t\t\tNO_valid_trial +=1\n",
        "\n",
        "\n",
        "\treturn data_return[0:NO_valid_trial,:,:], class_return[0:NO_valid_trial]\n",
        "\n",
        "def prepare_features(path,subject,crossValidation=False):\n",
        "    fs = 250 \n",
        "    t1 = int(3*fs)\n",
        "    t2 = int(7*fs)\n",
        "    T = t2-t1\n",
        "    X_train, y_train = get_data(subject+1,True,path)\n",
        "    if crossValidation:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=0)\n",
        "    else:\n",
        "        X_test, y_test = get_data(subject+1,False,path)\n",
        "\n",
        "    # prepare training data \t\n",
        "    N_tr,N_ch,_ =X_train.shape \n",
        "    X_train = X_train[:,:,t1:t2].reshape(N_tr,1,N_ch,T)\n",
        "    y_train_onehot = (y_train-1).astype(int)\n",
        "    y_train_onehot = to_categorical(y_train_onehot)\n",
        "    # prepare testing data \n",
        "    N_test,N_ch,_ =X_test.shape \n",
        "    X_test = X_test[:,:,t1:t2].reshape(N_test,1,N_ch,T)\n",
        "    y_test_onehot = (y_test-1).astype(int)\n",
        "    y_test_onehot = to_categorical(y_test_onehot)\t\n",
        "    return X_train,y_train,y_train_onehot,X_test,y_test,y_test_onehot"
      ],
      "metadata": {
        "id": "YRCtA1s0lTfR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network\n"
      ],
      "metadata": {
        "id": "F3--hc7U1kcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, classes_num):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.numC=classes_num\n",
        "\n",
        "        self.drop_out = 0.2\n",
        "        \n",
        "        self.block_1 = nn.Sequential(\n",
        "            # Pads the input tensor boundaries with zero\n",
        "            # left, right, up, bottom\n",
        "            nn.ZeroPad2d((15, 16, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,          # input shape (1, C, T)\n",
        "                out_channels=8,         # num_filters\n",
        "                kernel_size=(1, 32),    # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (8, C, T)\n",
        "            nn.BatchNorm2d(8)           # output shape (8, C, T)\n",
        "        )\n",
        "        \n",
        "        # block 2 and 3 are implementations of Depthwise Convolution and Separable Convolution\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=8,          # input shape (8, C, T)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(self.numC, 1),    # filter size\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//4)\n",
        "            nn.Dropout(self.drop_out)   # output shape (16, 1, T//4)\n",
        "        )\n",
        "        \n",
        "        self.block_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "               in_channels=16,          # input shape (16, 1, T//4)\n",
        "               out_channels=16,         # num_filters\n",
        "               kernel_size=(1, 16),     # filter size\n",
        "               groups=16,\n",
        "               bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,         # input shape (16, 1, T//4)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(1, 1),     # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T//4)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//32)\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        return x \n",
        "\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RelationNetwork(nn.Module):\n",
        "    \"\"\"docstring for RelationNetwork\"\"\"\n",
        "    def __init__(self,input_size,hidden_size):\n",
        "        super(RelationNetwork, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(32,16,kernel_size=(1,2),padding=1),\n",
        "                        nn.BatchNorm2d(16, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(16,16,kernel_size=(1,2),padding=1),\n",
        "                        nn.BatchNorm2d(16, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        \n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = torch.sigmoid(self.fc2(out))\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "Xju6ME7i1cFL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data generator"
      ],
      "metadata": {
        "id": "JLUS-CZXsCEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_set(label_set,num):\n",
        "  index1=np.where(label_set == 0)[0]\n",
        "  index2=np.where(label_set == 1)[0]\n",
        "  number=max(index1[num-1],index2[num-1])+1\n",
        "  return number"
      ],
      "metadata": {
        "id": "ARVU34KzVQDQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "class BCI_task(object):\n",
        "\n",
        "    def __init__(self, feature_list,label,train_num,test_num,settype):\n",
        "\n",
        "        self.train_num = train_num\n",
        "        self.test_num = test_num\n",
        "        if settype==\"test\":\n",
        "          testmode=mini_set(label,train_num)\n",
        "          # testmode=40\n",
        "          supportset=label[:testmode]\n",
        "          index1=np.where(supportset == 0)[0]\n",
        "          index2=np.where(supportset == 1)[0]\n",
        "          choice1=np.random.choice(index1,train_num,replace=False)\n",
        "          choice2=np.random.choice(index2,train_num,replace=False)\n",
        "          choice_total=np.hstack((choice1,choice2))\n",
        "          self.training_feature = feature_list[choice_total]\n",
        "          self.train_labels= label[choice_total]\n",
        "          test_list=list(range(testmode))\n",
        "\n",
        "          self.testing_feature= np.delete(feature_list, test_list,0)\n",
        "          self.test_labels=np.delete(label, test_list)\n",
        "\n",
        "\n",
        "        else:\n",
        "          index1=np.where(label == 0)[0]\n",
        "          index2=np.where(label == 1)[0]\n",
        "          choice1=np.random.choice(index1,train_num+test_num,replace=False)\n",
        "          choice2=np.random.choice(index2,train_num+test_num,replace=False)\n",
        "          train_choice=np.hstack((choice1[:train_num],choice2[:train_num]))\n",
        "          test_choice=np.hstack((choice1[train_num:],choice2[train_num:]))\n",
        "\n",
        "    \n",
        "\n",
        "          self.training_feature = feature_list[train_choice]\n",
        "          self.train_labels= label[train_choice]\n",
        "\n",
        "\n",
        "\n",
        "          self.testing_feature = feature_list[test_choice]\n",
        "          self.test_labels =label[test_choice]\n",
        "\n",
        "\n",
        "\n",
        "class FewShotDataset(Dataset):\n",
        "\n",
        "    def __init__(self, task, split='train'):\n",
        "        self.task = task\n",
        "        self.split = split\n",
        "        self.feature = self.task.training_feature if self.split == 'train' else self.task.testing_feature\n",
        "        self.labels = self.task.train_labels if self.split == 'train' else self.task.test_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raise NotImplementedError(\"This is an abstract class. Subclass this class for your particular dataset.\")\n",
        "\n",
        "\n",
        "class BCI_DATA(FewShotDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(BCI_DATA, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.feature[idx]\n",
        "        label = self.labels[idx]\n",
        "        return feature.astype(np.float32), label\n",
        "\n",
        "\n",
        "def get_data_loader(task, split='train',shuffle=False):\n",
        "    dataset = BCI_DATA(task,split=split)\n",
        "    number = len(task.training_feature) if split == 'train' else len(task.testing_feature)\n",
        "    loader = DataLoader(dataset, batch_size=number,shuffle=shuffle)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "E9lTuzqyQpaw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight initializer"
      ],
      "metadata": {
        "id": "GpzkQE4pa840"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        n = m.weight.size(1)\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data = torch.ones(m.bias.data.size())"
      ],
      "metadata": {
        "id": "-ytN9D43a_3Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter"
      ],
      "metadata": {
        "id": "-nCxxk1el1Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_se=10\n",
        "TRAIN_NUM=hyperparameter_se\n",
        "TEST_NUM=hyperparameter_se\n",
        "LEARNING_RATE=0.001\n",
        "FEATURE_DIM=64\n",
        "RELATION_DIM=8\n",
        "EPSIODE_NUM=10000"
      ],
      "metadata": {
        "id": "KEw-s9R4l3IM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "UND1ZLK4c_f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(Subject_number):\n",
        "\n",
        "\n",
        "  GPU=0\n",
        "\n",
        "  feature_encoder = EEGNet(classes_num=3)\n",
        "  relation_network = RelationNetwork(FEATURE_DIM,RELATION_DIM)\n",
        "\n",
        "  feature_encoder.apply(weights_init)\n",
        "  relation_network.apply(weights_init)\n",
        "\n",
        "  feature_encoder.cuda(GPU)\n",
        "  relation_network.cuda(GPU)\n",
        "\n",
        "\n",
        "  x_train,y_train,y_train_onehot,x_test,y_test,y_test_onehot = prepare_features('',Subject_number,False)\n",
        "  \n",
        "  xxxtest=np.vstack((x_train,x_test))\n",
        "  yyytest=np.hstack((y_train,y_test))\n",
        "\n",
        "\n",
        "\n",
        "  total_x_train=[]\n",
        "  total_y_train=[]\n",
        "  full_subjectnum=[0,1,2,3,4,5,6,7,8]\n",
        "  full_subjectnum.remove(Subject_number)\n",
        "  total_losss=[]\n",
        "  \n",
        "  for train_subject in full_subjectnum:\n",
        "    x_train,y_train,y_train_onehot,x_test,y_test,y_test_onehot = prepare_features('',train_subject,False)\n",
        "\n",
        "    if len(total_x_train)==0:\n",
        "      total_x_train=x_train\n",
        "      total_y_train=y_train\n",
        "    else:\n",
        "      total_x_train=np.vstack((total_x_train,x_train))\n",
        "      total_y_train=np.hstack((total_y_train,y_train))\n",
        "    total_x_train=np.vstack((total_x_train,x_test))\n",
        "    total_y_train=np.hstack((total_y_train,y_test))\n",
        "  x_train=total_x_train\n",
        "  y_train=total_y_train-1\n",
        "  x_test=xxxtest\n",
        "  y_test=yyytest-1\n",
        "\n",
        "  feature_encoder_optim = torch.optim.Adam(feature_encoder.parameters(),lr=LEARNING_RATE)\n",
        "  relation_network_optim = torch.optim.Adam(relation_network.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "  testlossset=[]\n",
        "  for episode in range(EPSIODE_NUM):\n",
        "    bci_task=BCI_task(x_train,y_train,train_num=TRAIN_NUM,test_num=TEST_NUM,settype='train')\n",
        "    sample_dataloader = get_data_loader(bci_task,split=\"train\",shuffle=False)\n",
        "    batch_dataloader = get_data_loader(bci_task,split=\"test\",shuffle=False)\n",
        "\n",
        "    samples,sample_labels = sample_dataloader.__iter__().next()\n",
        "    batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "\n",
        "    ### \n",
        "    #Directly calculate each score\n",
        "    ##\n",
        " \n",
        "    #sum method pending to write\n",
        "    sample_labels=sample_labels.type(torch.int32).cuda(GPU)\n",
        "    sample_features = feature_encoder(Variable(samples).cuda(GPU)) # 10x16*1*4\n",
        "    nrow = torch.unique(sample_labels).size(0)\n",
        "    out = torch.zeros((nrow, sample_features.size(1),sample_features.size(2),sample_features.size(3)), dtype=sample_features.dtype).cuda(GPU)\n",
        "    out.index_add_(0, sample_labels, sample_features)\n",
        "    sample_features=out    # 2x16*1*4\n",
        "    batch_features = feature_encoder(Variable(batches).cuda(GPU)) # 5x16*1*4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    sample_features_ext = sample_features.unsqueeze(0).repeat(len(bci_task.testing_feature),1,1,1,1)\n",
        "    batch_features_ext = batch_features.unsqueeze(0).repeat(sample_features.size(0),1,1,1,1)\n",
        "    batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "\n",
        "\n",
        "    relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,batch_features_ext.size()[2]*2,batch_features_ext.size()[3],batch_features_ext.size()[4])\n",
        "    relations = relation_network(relation_pairs)\n",
        "    relations = relations.view(len(bci_task.testing_feature),-1)\n",
        "    mse = nn.MSELoss(reduction='mean').cuda(GPU)\n",
        "    one_hot_labels =F.one_hot(batch_labels.to(torch.int64), num_classes=2).float().cuda(GPU)\n",
        "\n",
        "    loss = mse(relations,one_hot_labels)\n",
        "\n",
        "    feature_encoder.zero_grad()\n",
        "    relation_network.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(feature_encoder.parameters(),0.5)\n",
        "    torch.nn.utils.clip_grad_norm_(relation_network.parameters(),0.5)\n",
        "\n",
        "    feature_encoder_optim.step()\n",
        "    relation_network_optim.step()\n",
        "    total_losss.append(loss.item())\n",
        "\n",
        "\n",
        "\n",
        "    if (episode+1)%1000 == 0:\n",
        "        # print(\"episode:\",episode+1,\"loss\",np.mean(total_losss))\n",
        "        total_losss=[]\n",
        "        total_rewards = 0\n",
        "        bci_task=BCI_task(x_test,y_test,train_num=TRAIN_NUM,test_num=TEST_NUM,settype='test')\n",
        "        sample_dataloader = get_data_loader(bci_task,split=\"train\",shuffle=False)\n",
        "        batch_dataloader = get_data_loader(bci_task,split=\"test\",shuffle=False)\n",
        "\n",
        "        samples,sample_labels = sample_dataloader.__iter__().next()\n",
        "        batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "\n",
        "\n",
        "      # print(batch_labels)\n",
        "    #### \n",
        "    # Directly calculate each score\n",
        "    ###\n",
        " \n",
        "    # sum method pending to write\n",
        "        sample_labels=sample_labels.type(torch.int32).cuda(GPU)\n",
        "        sample_features = feature_encoder(Variable(samples).cuda(GPU))\n",
        "        nrow = torch.unique(sample_labels).size(0)\n",
        "        out = torch.zeros((nrow, sample_features.size(1),sample_features.size(2),sample_features.size(3)), dtype=sample_features.dtype).cuda(GPU)\n",
        "        out.index_add_(0, sample_labels, sample_features)\n",
        "        sample_features=out    # 2x16*1*4\n",
        "        batch_features = feature_encoder(Variable(batches).cuda(GPU)) \n",
        "\n",
        "\n",
        "\n",
        "        sample_features_ext = sample_features.unsqueeze(0).repeat(len(bci_task.testing_feature),1,1,1,1)\n",
        "        batch_features_ext = batch_features.unsqueeze(0).repeat(sample_features.size(0),1,1,1,1)\n",
        "        batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "        relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,batch_features_ext.size()[2]*2,batch_features_ext.size()[3],batch_features_ext.size()[4])\n",
        "        relations = relation_network(relation_pairs)\n",
        "        relations = relations.view(len(bci_task.testing_feature),-1)\n",
        "        _,predict_labels = torch.max(relations.data,1)\n",
        "        rewards = [1 if predict_labels[j]==batch_labels[j] else 0 for j in range(len(bci_task.testing_feature))]\n",
        "\n",
        "        testlossset.append(np.sum(rewards)/len(rewards)*100)\n",
        "\n",
        "  print(\"Last episode test accuracy:\",np.sum(rewards)/len(rewards)*100,\"%\")\n",
        "  print(\"Highest three each 1000 episodes:\",np.sort(testlossset)[-3:])\n"
      ],
      "metadata": {
        "id": "xVeqn3Cnz0b7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the subject number 0-8 corrsponding to subject B01-B09; Subject_number=0\n",
        "\n",
        "subject_number=3\n",
        "main(subject_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLJcTOXZlo3a",
        "outputId": "2855a02a-2d87-4edf-b3b5-310b85c8609b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last episode test accuracy: 89.65014577259474 %\n",
            "Highest three each 1000 episodes: [90.37900875 90.52478134 90.96209913]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KKMf9CoZP7-c"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}