{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_B_III_IVa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction"
      ],
      "metadata": {
        "id": "f6olkENpZyye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run each cell one by one**"
      ],
      "metadata": {
        "id": "OE96pIfnZy2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n"
      ],
      "metadata": {
        "id": "JYCCkAM20MYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install mne\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import mne\n",
        "def indexget(total,select):\n",
        "  return [i for i, e in enumerate(total) if e in select]\n",
        "\n",
        "\n",
        "\n",
        "def inputmat(fp1,fp2,fp3):\n",
        "    \"\"\"load .mat file and return m as a dict\"\"\"\n",
        "    mat = sio.loadmat(fp1, squeeze_me=True)\n",
        "    label_mat=sio.loadmat(fp2, squeeze_me=True)\n",
        "    m = {}  # create a dict\n",
        "\n",
        "    # Numpy array of size channel_num * points.\n",
        "    select_channel=fp3\n",
        "    channelindex= indexget(mat['nfo']['clab'][True][0],select_channel)\n",
        "    m['ch_names'] = mat['nfo']['clab'][True][0][channelindex]\n",
        "    \n",
        "\n",
        "\n",
        "    m['data'] = mat['cnt'].T[channelindex]\n",
        "    m['freq'] = mat['nfo']['fs'][True][0]  # Sampling frequency\n",
        "\n",
        "    # channel names are necessary information for creating a rawArray.\n",
        "\n",
        "\n",
        "    # Position of channels\n",
        "    m['electrode_x'] = mat['nfo']['xpos'][True][0]\n",
        "    m['electrode_y'] = mat['nfo']['ypos'][True][0]\n",
        "\n",
        "    # find trials and its data\n",
        "    m['cue'] = mat['mrk']['pos'][True][0]  # time of cue\n",
        "    m['labels'] = label_mat['true_y']  \n",
        "    # m['labels'] = np.nan_to_num(mat['mrk']['y'][True][0]).astype(int)  # convert NaN to 0\n",
        "    m['test_idx'] = len(label_mat['test_idx'])\n",
        "    m['n_trials'] = len(m['labels'])  # Number of the total useful trials\n",
        "    return m\n",
        "\n",
        "\n",
        "def creatEventsArray(fp1,fp2,fp3):\n",
        "    \"\"\"Create events array. The second column default to zero.\"\"\"\n",
        "    m = inputmat(fp1,fp2,fp3)\n",
        "    events = np.zeros((m['n_trials'], 3), int)\n",
        "    events[:, 0] = m['cue'][:m['n_trials']]  # The first column is the sample number of the event.\n",
        "    events[:, 2] = m['labels'][:m['n_trials']]  # The third column is the new event value.\n",
        "    return events, m['labels']\n",
        "\n",
        "\n",
        "def creatRawArray(fp1,fp2,fp3):\n",
        "    \"\"\"Create a mne.io.RawArray object, data: array, shape (n_channels, n_times)\"\"\"\n",
        "    m = inputmat(fp1,fp2,fp3)\n",
        "    ch_names = m['ch_names'].tolist()\n",
        "    info = mne.create_info(ch_names, m['freq'], 'eeg')  # Create info for raw\n",
        "    raw = mne.io.RawArray(m['data'], info, first_samp=0, copy='auto', verbose=None)\n",
        "    return raw"
      ],
      "metadata": {
        "id": "Ruyr8-MtlIB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f9fdc4-653b-440c-9cc0-cb2d68bc2bac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mne\n",
            "  Downloading mne-1.0.3-py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package"
      ],
      "metadata": {
        "id": "BrAnxBIalLoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd.function import Function\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.utils.data as Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n"
      ],
      "metadata": {
        "id": "8bvfxtJilOo-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "atSHs5e7lRuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(__doc__)\n",
        "\n",
        "fp = {\n",
        "    'aa': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/data_set_IVa_aa.mat',\n",
        "    'al': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/data_set_IVa_al.mat',\n",
        "    'av': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/data_set_IVa_av.mat',\n",
        "    'aw': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/data_set_IVa_aw.mat',\n",
        "    'ay': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/data_set_IVa_ay.mat',\n",
        "}\n",
        "\n",
        "\n",
        "fplabel={\n",
        "    'aa': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/true_labels_aa.mat',\n",
        "    'al': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/true_labels_al.mat',\n",
        "    'av': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/true_labels_av.mat',\n",
        "    'aw': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/true_labels_aw.mat',\n",
        "    'ay': '/content/drive/MyDrive/fewshotonlineBCI/CompetitionIII_IVa/true_labels_ay.mat',\n",
        "\n",
        "}\n",
        "\n",
        "channel_set=['C1',  'C3',  'Cz',  'C2',  'C4',  'CFC3',  'CFC4','CFC5',  'CFC6',  'CCP3', \n",
        "             'CCP4',  'CCP5',  'CCP6',  'T7',  'T8',  'P3',  'Pz',  'P4']\n",
        "pick_chan = {\n",
        "    'aa':  channel_set,\n",
        "    'al':  channel_set,\n",
        "    'av':  channel_set,\n",
        "    'aw':  channel_set,\n",
        "    'ay':  channel_set,\n",
        "\n",
        "}\n",
        "\n",
        "low_freq, high_freq = 7., 30.\n",
        "tmin, tmax = 0., 3.5\n",
        "# event_id\n",
        "event_id = {'right': 1, 'foot': 2}"
      ],
      "metadata": {
        "id": "YRCtA1s0lTfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8402d3f-a0ea-439a-cfc8-ee1ef0c50c1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from mne.decoding import CSP\n",
        "from mne.channels import read_layout\n",
        "\n",
        "\n",
        "\n",
        "def cal_acc(pred,real):\n",
        "  return [1 if pred[j]==real[j] else 0 for j in range(len(real))]\n",
        "\n",
        "def getdata_label(a,b,c):\n",
        "  mat_dic=inputmat(a,b,c)\n",
        "  raw = creatRawArray(a,b,c)\n",
        "  events, labels = creatEventsArray(a,b,c)\n",
        "\n",
        "\n",
        "    # Apply band-pass filter\n",
        "  raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge')\n",
        "\n",
        "    # event_train = eventsTrain(fp[f])\n",
        "  epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True,\n",
        "                        verbose=False)\n",
        "\n",
        "  epochs_train = epochs.copy().crop(tmin=0.5, tmax=2.5)\n",
        "  labels = epochs.events[:, -1]\n",
        "\n",
        "    # # Define a monte-carlo cross-validation generator (reduce variance):\n",
        "  epochs_data= epochs_train.get_data()\n",
        "  return epochs_data,labels,mat_dic['test_idx']\n",
        "\n",
        "intial_dic={}\n",
        "\n",
        "total_length=280\n",
        "train_num=10\n",
        "test_num=10\n",
        "\n",
        "for f,fla,cs in zip(fp,fplabel,pick_chan):\n",
        "    x,y,testix=getdata_label(fp[f],fplabel[fla],pick_chan[cs])\n",
        "\n",
        "    intial_dic[f]=[x,y]\n",
        "    # # Assemble a classifier\n",
        "    # \n",
        "    # csp = CSP(n_components=len(epochs.ch_names), reg=None, log=True, norm_trace=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMzDePH9nDFJ",
        "outputId": "27afc0cf-c828-4e60-c40e-7663157ec023"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RawArray with float64 data, n_channels=18, n_times=298458\n",
            "    Range : 0 ... 298457 =      0.000 ...  2984.570 secs\n",
            "Ready.\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 7 - 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 7.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 165 samples (1.650 sec)\n",
            "\n",
            "Creating RawArray with float64 data, n_channels=18, n_times=283574\n",
            "    Range : 0 ... 283573 =      0.000 ...  2835.730 secs\n",
            "Ready.\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 7 - 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 7.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 165 samples (1.650 sec)\n",
            "\n",
            "Creating RawArray with float64 data, n_channels=18, n_times=283042\n",
            "    Range : 0 ... 283041 =      0.000 ...  2830.410 secs\n",
            "Ready.\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 7 - 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 7.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 165 samples (1.650 sec)\n",
            "\n",
            "Creating RawArray with float64 data, n_channels=18, n_times=282838\n",
            "    Range : 0 ... 282837 =      0.000 ...  2828.370 secs\n",
            "Ready.\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 7 - 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 7.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 165 samples (1.650 sec)\n",
            "\n",
            "Creating RawArray with float64 data, n_channels=18, n_times=283562\n",
            "    Range : 0 ... 283561 =      0.000 ...  2835.610 secs\n",
            "Ready.\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 7 - 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 7.00\n",
            "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 165 samples (1.650 sec)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network\n"
      ],
      "metadata": {
        "id": "F3--hc7U1kcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "hyperparameter_se=10\n",
        "\n",
        "\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, classes_num):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.numC=classes_num\n",
        "\n",
        "        self.drop_out = 0\n",
        "        \n",
        "        self.block_1 = nn.Sequential(\n",
        "            # Pads the input tensor boundaries with zero\n",
        "            # left, right, up, bottom\n",
        "            nn.ZeroPad2d((15, 16, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,          # input shape (1, C, T)\n",
        "                out_channels=8,         # num_filters\n",
        "                kernel_size=(1, 32),    # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (8, C, T)\n",
        "            nn.BatchNorm2d(8)           # output shape (8, C, T)\n",
        "        )\n",
        "        \n",
        "        # block 2 and 3 are implementations of Depthwise Convolution and Separable Convolution\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=8,          # input shape (8, C, T)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(self.numC, 1),    # filter size\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//4)\n",
        "            nn.Dropout(self.drop_out)   # output shape (16, 1, T//4)\n",
        "        )\n",
        "        \n",
        "        self.block_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "               in_channels=16,          # input shape (16, 1, T//4)\n",
        "               out_channels=16,         # num_filters\n",
        "               kernel_size=(1, 16),     # filter size\n",
        "               groups=16,\n",
        "               bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,         # input shape (16, 1, T//4)\n",
        "                out_channels=16,        # num_filters\n",
        "                kernel_size=(1, 1),     # filter size\n",
        "                bias=False\n",
        "            ),                          # output shape (16, 1, T//4)\n",
        "            nn.BatchNorm2d(16),         # output shape (16, 1, T//4)\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8)),       # output shape (16, 1, T//32)\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        return x \n",
        "\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RelationNetwork(nn.Module):\n",
        "    \"\"\"docstring for RelationNetwork\"\"\"\n",
        "    def __init__(self,hidden_size):\n",
        "        super(RelationNetwork, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(32,16,kernel_size=(1,2),padding=1),\n",
        "                        nn.BatchNorm2d(16, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(16,16,kernel_size=(1,2),padding=1),\n",
        "                        nn.BatchNorm2d(16, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(32,hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=x.view(-1,32,1,3)\n",
        "        out = self.layer1(x)\n",
        "        # out = self.layer2(out)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        \n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = torch.sigmoid(self.fc2(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "Xju6ME7i1cFL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data generator"
      ],
      "metadata": {
        "id": "JLUS-CZXsCEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_set(label_set,num):\n",
        "  index1=np.where(label_set == 0)[0]\n",
        "  index2=np.where(label_set == 1)[0]\n",
        "  number=max(index1[num-1],index2[num-1])+1\n",
        "  return number\n",
        "  \n"
      ],
      "metadata": {
        "id": "3ZPyPsL0YAJG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "class BCI_task(object):\n",
        "\n",
        "    def __init__(self, feature_list,label,train_num,test_num,settype):\n",
        "\n",
        "        self.train_num = train_num\n",
        "        self.test_num = test_num\n",
        "        if settype==\"test\":\n",
        "          testmode=mini_set(label,train_num)\n",
        "          # testmode=40\n",
        "          supportset=label[:testmode]\n",
        "          index1=np.where(supportset == 0)[0]\n",
        "          index2=np.where(supportset == 1)[0]\n",
        "          choice1=np.random.choice(index1,train_num,replace=False)\n",
        "          choice2=np.random.choice(index2,train_num,replace=False)\n",
        "          choice_total=np.hstack((choice1,choice2))\n",
        "          self.training_feature = feature_list[choice_total]\n",
        "          self.train_labels= label[choice_total]\n",
        "          test_list=list(range(testmode))\n",
        "\n",
        "          self.testing_feature= np.delete(feature_list, test_list,0)\n",
        "          self.test_labels=np.delete(label, test_list)\n",
        "\n",
        "        else:\n",
        "          index1=np.where(label == 0)[0]\n",
        "          index2=np.where(label == 1)[0]\n",
        "          choice1=np.random.choice(index1,train_num+test_num,replace=False)\n",
        "          choice2=np.random.choice(index2,train_num+test_num,replace=False)\n",
        "          train_choice=np.hstack((choice1[:train_num],choice2[:train_num]))\n",
        "          test_choice=np.hstack((choice1[train_num:],choice2[train_num:]))\n",
        "\n",
        "    \n",
        "\n",
        "          self.training_feature = feature_list[train_choice]\n",
        "          self.train_labels= label[train_choice]\n",
        "\n",
        "\n",
        "\n",
        "          self.testing_feature = feature_list[test_choice]\n",
        "          self.test_labels =label[test_choice]\n",
        "\n",
        "\n",
        "\n",
        "class FewShotDataset(Dataset):\n",
        "\n",
        "    def __init__(self, task, split='train'):\n",
        "        self.task = task\n",
        "        self.split = split\n",
        "        self.feature = self.task.training_feature if self.split == 'train' else self.task.testing_feature\n",
        "        self.labels = self.task.train_labels if self.split == 'train' else self.task.test_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raise NotImplementedError(\"This is an abstract class. Subclass this class for your particular dataset.\")\n",
        "\n",
        "\n",
        "class BCI_DATA(FewShotDataset):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(BCI_DATA, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.feature[idx]\n",
        "        label = self.labels[idx]\n",
        "        return feature.astype(np.float32), label\n",
        "\n",
        "\n",
        "def get_data_loader(task, split='train',shuffle=False):\n",
        "    dataset = BCI_DATA(task,split=split)\n",
        "    number = len(task.training_feature) if split == 'train' else len(task.testing_feature)\n",
        "    loader = DataLoader(dataset, batch_size=number,shuffle=shuffle)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "E9lTuzqyQpaw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight initializer"
      ],
      "metadata": {
        "id": "GpzkQE4pa840"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        n = m.weight.size(1)\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data = torch.ones(m.bias.data.size())"
      ],
      "metadata": {
        "id": "-ytN9D43a_3Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter"
      ],
      "metadata": {
        "id": "-nCxxk1el1Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_NUM=hyperparameter_se\n",
        "TEST_NUM=hyperparameter_se\n",
        "LEARNING_RATE=0.001\n",
        "\n",
        "EPSIODE_NUM=2000\n",
        "\n",
        "FEATURE_DIM=240\n",
        "RELATION_DIM=240\n",
        "n_layers=1\n",
        "MIDDLE_DIM=240"
      ],
      "metadata": {
        "id": "KEw-s9R4l3IM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "UND1ZLK4c_f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic=[]\n",
        "str_list=['aa','al','av','aw','ay']\n",
        "\n",
        "\n",
        "for i in str_list:\n",
        "  temp=[]\n",
        "  x,y=intial_dic[i]\n",
        "  x=x.reshape(x.shape[0],1,x.shape[1],-1)\n",
        "\n",
        "  y=y-1\n",
        "  temp.append(x)\n",
        "  temp.append(y)\n",
        "\n",
        "  dic.append(temp)\n",
        "\n"
      ],
      "metadata": {
        "id": "D_OXX6kLn44n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(Subject_number):\n",
        "\n",
        "\n",
        "  GPU=0\n",
        "  feature_encoder = EEGNet(classes_num=18)\n",
        "  relation_network = RelationNetwork(FEATURE_DIM)\n",
        "\n",
        "  feature_encoder.apply(weights_init)\n",
        "  relation_network.apply(weights_init)\n",
        "  GPU=0\n",
        "\n",
        "  feature_encoder.cuda(GPU)\n",
        "  relation_network.cuda(GPU)\n",
        "\n",
        "\n",
        "\n",
        "  x_test=dic[Subject_number][0]\n",
        "  y_test=dic[Subject_number][1]\n",
        "\n",
        "\n",
        "  full_subjectnum=[0,1,2,3,4]\n",
        "  full_subjectnum.remove(Subject_number)\n",
        "\n",
        "  feature_encoder_optim = torch.optim.Adam(feature_encoder.parameters(),lr=LEARNING_RATE)\n",
        "  relation_network_optim = torch.optim.Adam(relation_network.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "  total_losss=[]\n",
        "  testlossset=[]\n",
        "  for episode in range(EPSIODE_NUM):\n",
        "    select_subject=np.random.choice(full_subjectnum, 1)[0]\n",
        "    x_train,y_train = dic[select_subject][0],dic[select_subject][1]\n",
        "\n",
        "\n",
        "    bci_task=BCI_task(x_train,y_train,train_num=TRAIN_NUM,test_num=TEST_NUM,settype='train')\n",
        "    sample_dataloader = get_data_loader(bci_task,split=\"train\",shuffle=False)\n",
        "    batch_dataloader = get_data_loader(bci_task,split=\"test\",shuffle=False)\n",
        "\n",
        "    samples,sample_labels = sample_dataloader.__iter__().next()\n",
        "    batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "\n",
        "    ### \n",
        "    #Directly calculate each score\n",
        "    ##\n",
        " \n",
        "    #sum method pending to write\n",
        "    sample_labels=sample_labels.type(torch.int32).cuda(GPU)\n",
        "    sample_features = feature_encoder(Variable(samples).cuda(GPU)) # 10x16*1*4\n",
        "    nrow = torch.unique(sample_labels).size(0)\n",
        "    out = torch.zeros((nrow, sample_features.size(1),sample_features.size(2),sample_features.size(3)), dtype=sample_features.dtype).cuda(GPU)\n",
        "    out.index_add_(0, sample_labels, sample_features)\n",
        "    sample_features=out    # 2x16*1*4\n",
        "    batch_features = feature_encoder(Variable(batches).cuda(GPU)) # 5x16*1*4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    sample_features_ext = sample_features.unsqueeze(0).repeat(len(bci_task.testing_feature),1,1,1,1)\n",
        "    batch_features_ext = batch_features.unsqueeze(0).repeat(sample_features.size(0),1,1,1,1)\n",
        "    batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "\n",
        "\n",
        "    relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,batch_features_ext.size()[2]*2,batch_features_ext.size()[3],batch_features_ext.size()[4])\n",
        "    relations = relation_network(relation_pairs)\n",
        "    relations = relations.view(len(bci_task.testing_feature),-1)\n",
        "    mse = nn.MSELoss(reduction='mean').cuda(GPU)\n",
        "    one_hot_labels =F.one_hot(batch_labels.to(torch.int64), num_classes=2).float().cuda(GPU)\n",
        "\n",
        "    loss = mse(relations,one_hot_labels)\n",
        "\n",
        "    feature_encoder.zero_grad()\n",
        "    relation_network.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(feature_encoder.parameters(),0.5)\n",
        "    torch.nn.utils.clip_grad_norm_(relation_network.parameters(),0.5)\n",
        "\n",
        "    feature_encoder_optim.step()\n",
        "    relation_network_optim.step()\n",
        "    total_losss.append(loss.item())\n",
        "\n",
        "\n",
        "\n",
        "    if (episode+1)%200 == 0:\n",
        "        total_losss=[]\n",
        "        total_rewards = 0\n",
        "        bci_task=BCI_task(x_test,y_test,train_num=TRAIN_NUM,test_num=TEST_NUM,settype='test')\n",
        "        sample_dataloader = get_data_loader(bci_task,split=\"train\",shuffle=False)\n",
        "        batch_dataloader = get_data_loader(bci_task,split=\"test\",shuffle=False)\n",
        "\n",
        "        samples,sample_labels = sample_dataloader.__iter__().next()\n",
        "        batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "        sample_labels=sample_labels.type(torch.int32).cuda(GPU)\n",
        "        sample_features = feature_encoder(Variable(samples).cuda(GPU))\n",
        "        nrow = torch.unique(sample_labels).size(0)\n",
        "        out = torch.zeros((nrow, sample_features.size(1),sample_features.size(2),sample_features.size(3)), dtype=sample_features.dtype).cuda(GPU)\n",
        "        out.index_add_(0, sample_labels, sample_features)\n",
        "        sample_features=out    # 2x16*1*4\n",
        "        batch_features = feature_encoder(Variable(batches).cuda(GPU)) \n",
        "\n",
        "\n",
        "\n",
        "        sample_features_ext = sample_features.unsqueeze(0).repeat(len(bci_task.testing_feature),1,1,1,1)\n",
        "        batch_features_ext = batch_features.unsqueeze(0).repeat(sample_features.size(0),1,1,1,1)\n",
        "        batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "        relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,batch_features_ext.size()[2]*2,batch_features_ext.size()[3],batch_features_ext.size()[4])\n",
        "        relations = relation_network(relation_pairs)\n",
        "        relations = relations.view(len(bci_task.testing_feature),-1)\n",
        "        _,predict_labels = torch.max(relations.data,1)\n",
        "        rewards = [1 if predict_labels[j]==batch_labels[j] else 0 for j in range(len(bci_task.testing_feature))]\n",
        "        testlossset.append(np.sum(rewards)/len(rewards)*100)\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Last episode test accuracy:\",np.sum(rewards)/len(rewards)*100,\"%\")\n",
        "  print(\"Highest three each 200 episodes:\",np.sort(testlossset)[-3:])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xVeqn3Cnz0b7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Subject_number=2\n",
        "main(Subject_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLJcTOXZlo3a",
        "outputId": "1089cd50-53d1-4835-ba06-b0a6e875a0d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last episode test accuracy: 59.45945945945946 %\n",
            "Highest three each 200 episodes: [63.70656371 65.25096525 66.7953668 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PQPm_OCocDNK"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}